Loss: 42.70774459838867
Loss: 42.67097473144531
Loss: 42.72358703613281
Loss: 42.78444290161133
Loss: 42.607540130615234
Loss: 42.642757415771484
Loss: 42.50127029418945
Loss: 42.61322021484375
Loss: 42.63037872314453
Loss: 42.435115814208984
Loss: 42.170352935791016
Loss: 41.9781608581543
Loss: 41.95619583129883
Loss: 42.09046936035156
Loss: 42.0889892578125
Loss: 41.755149841308594
Loss: 41.71814727783203
Loss: 41.36833572387695
Loss: 41.39713668823242
Loss: 41.008567810058594
Loss: 40.877227783203125
Loss: 41.08061218261719
Loss: 40.80314254760742
Loss: 40.641788482666016
Loss: 40.233795166015625
Loss: 40.613162994384766
Loss: 40.31717300415039
Loss: 40.35346603393555
Loss: 40.47703170776367
Loss: 40.00360107421875
Loss: 39.8449592590332
Loss: 39.91969299316406
Loss: 40.05275344848633
Loss: 39.73976135253906
Loss: 39.31687545776367
Loss: 39.68304443359375
Loss: 39.224708557128906
Loss: 39.26750183105469
Loss: 39.1293830871582
Loss: 39.4883918762207
Loss: 39.09758758544922
Loss: 39.0078010559082
Loss: 39.208534240722656
Loss: 38.85776138305664
Loss: 38.88010025024414
[Train] Epoch 1, accuracy 0.0022569444444444442
[Eval] Epoch 1, accuracy 0.002777777777777778
Model saved as model_weights_best.pth
Loss: 38.642242431640625
Loss: 38.708343505859375
Loss: 38.33879089355469
Loss: 38.331172943115234
Loss: 38.365379333496094
Loss: 38.21852111816406
Loss: 38.41770935058594
Loss: 38.41497802734375
Loss: 38.411556243896484
Loss: 38.1201286315918
Loss: 38.017433166503906
Loss: 38.16343307495117
Loss: 38.15973663330078
Loss: 38.05000686645508
Loss: 37.93408203125
Loss: 37.909217834472656
Loss: 37.90736389160156
Loss: 37.771644592285156
Loss: 37.65715408325195
Loss: 37.7884635925293
Loss: 37.87472152709961
Loss: 37.791873931884766
Loss: 37.62565994262695
Loss: 37.86870574951172
Loss: 37.544044494628906
Loss: 37.62135314941406
Loss: 37.57284927368164
Loss: 37.738502502441406
Loss: 37.360843658447266
Loss: 37.51383590698242
Loss: 37.577110290527344
Loss: 37.37142562866211
Loss: 37.42967224121094
Loss: 37.23810577392578
Loss: 37.295936584472656
Loss: 37.33931350708008
Loss: 37.280059814453125
Loss: 37.148399353027344
Loss: 37.24150085449219
Loss: 37.230350494384766
Loss: 37.366519927978516
Loss: 37.32933044433594
Loss: 37.26383972167969
Loss: 37.26993942260742
Loss: 37.19626998901367
[Train] Epoch 2, accuracy 0.0025173611111111113
[Eval] Epoch 2, accuracy 0.0038194444444444443
Model saved as model_weights_best.pth
Loss: 37.15109634399414
Loss: 37.14463806152344
Loss: 37.048160552978516
Loss: 37.09888458251953
Loss: 37.02412414550781
Loss: 37.17845916748047
Loss: 37.04488754272461
Loss: 37.1370849609375
Loss: 36.91598129272461
Loss: 37.16717529296875
Loss: 37.05626678466797
Loss: 37.07366943359375
Loss: 37.05046844482422
Loss: 37.0982551574707
Loss: 36.988319396972656
Loss: 37.01970672607422
Loss: 37.04182815551758
Loss: 37.05621337890625
Loss: 37.07035827636719
Loss: 37.03963088989258
Loss: 37.00984191894531
Loss: 37.08568572998047
Loss: 37.031578063964844
Loss: 36.94007110595703
Loss: 36.93374252319336
Loss: 37.123287200927734
Loss: 36.91358947753906
Loss: 37.014827728271484
Loss: 36.99913024902344
Loss: 37.05025100708008
Loss: 36.854820251464844
Loss: 36.928550720214844
Loss: 36.9128303527832
Loss: 37.00139617919922
Loss: 36.996829986572266
Loss: 36.95062255859375
Loss: 36.959014892578125
Loss: 36.93119812011719
Loss: 36.96489334106445
Loss: 36.940032958984375
Loss: 36.931739807128906
Loss: 37.03529739379883
Loss: 37.03334426879883
Loss: 37.03415298461914
Loss: 36.97248458862305
[Train] Epoch 3, accuracy 0.0030381944444444445
[Eval] Epoch 3, accuracy 0.005555555555555556
Model saved as model_weights_best.pth
Loss: 36.808143615722656
Loss: 36.89418411254883
Loss: 36.94022750854492
Loss: 37.09947967529297
Loss: 36.968711853027344
Loss: 37.02372741699219
Loss: 36.97562026977539
Loss: 36.891414642333984
Loss: 36.92196273803711
Loss: 36.95207214355469
Loss: 36.979949951171875
Loss: 36.89482498168945
Loss: 36.988807678222656
Loss: 36.98457336425781
Loss: 36.89838409423828
Loss: 36.97614669799805
Loss: 36.924434661865234
Loss: 36.864688873291016
Loss: 36.83402633666992
Loss: 36.94810104370117
Loss: 36.95159149169922
Loss: 36.884124755859375
Loss: 36.95304870605469
Loss: 36.97857666015625
Loss: 36.85512924194336
Loss: 37.06791687011719
Loss: 36.89301681518555
Loss: 36.8155632019043
Loss: 36.987972259521484
Loss: 36.86298751831055
Loss: 36.97184753417969
Loss: 36.915889739990234
Loss: 36.94337844848633
Loss: 36.922210693359375
Loss: 36.944820404052734
Loss: 36.95477294921875
Loss: 36.94525146484375
Loss: 36.87416076660156
Loss: 36.8339729309082
Loss: 36.86147689819336
Loss: 36.87813949584961
Loss: 36.919246673583984
Loss: 36.84844207763672
Loss: 36.876014709472656
Loss: 36.86502456665039
[Train] Epoch 4, accuracy 0.006510416666666667
[Eval] Epoch 4, accuracy 0.005902777777777778
Model saved as model_weights_best.pth
Loss: 36.785606384277344
Loss: 36.73062515258789
Loss: 36.85860061645508
Loss: 36.8202018737793
Loss: 36.680946350097656
Loss: 36.80340576171875
Loss: 36.87849807739258
Loss: 36.8775520324707
Loss: 36.81380844116211
Loss: 36.84010696411133
Loss: 36.76152801513672
Loss: 36.867794036865234
Loss: 36.82207107543945
Loss: 36.76157760620117
Loss: 36.844974517822266
Loss: 36.76323318481445
Loss: 36.84496307373047
Loss: 36.8640022277832
Loss: 36.737545013427734
Loss: 36.79960632324219
Loss: 36.904170989990234
Loss: 36.903900146484375
Loss: 36.88456344604492
Loss: 36.836265563964844
Loss: 36.86964797973633
Loss: 36.79410934448242
Loss: 36.87275314331055
Loss: 36.80592727661133
Loss: 36.80756759643555
Loss: 36.72140121459961
Loss: 36.73732376098633
Loss: 36.845821380615234
Loss: 36.91664123535156
Loss: 36.80433654785156
Loss: 36.81077194213867
Loss: 36.90318298339844
Loss: 36.9276123046875
Loss: 36.8830680847168
Loss: 36.75032043457031
Loss: 36.747413635253906
Loss: 36.772605895996094
Loss: 36.83047103881836
Loss: 36.96404266357422
Loss: 36.749080657958984
Loss: 36.759700775146484
[Train] Epoch 5, accuracy 0.010850694444444444
[Eval] Epoch 5, accuracy 0.008333333333333333
Model saved as model_weights_best.pth
Loss: 36.8631706237793
Loss: 36.81420135498047
Loss: 36.72590255737305
Loss: 36.818077087402344
Loss: 36.75727462768555
Loss: 36.778717041015625
Loss: 36.67992401123047
Loss: 36.826168060302734
Loss: 36.7335319519043
Loss: 36.77230453491211
Loss: 36.773536682128906
Loss: 36.75511932373047
Loss: 36.79659652709961
Loss: 36.74388122558594
Loss: 36.80201721191406
Loss: 36.844966888427734
Loss: 36.740787506103516
Loss: 36.72651672363281
Loss: 36.78290557861328
Loss: 36.655635833740234
Loss: 36.75823974609375
Loss: 36.72178649902344
Loss: 36.84405517578125
Loss: 36.766021728515625
Loss: 36.68157958984375
Loss: 36.76429748535156
Loss: 36.81991958618164
Loss: 36.69450378417969
Loss: 36.85730743408203
Loss: 36.82270050048828
Loss: 36.70487976074219
Loss: 36.74909210205078
Loss: 36.632041931152344
Loss: 36.684791564941406
Loss: 36.58365249633789
Loss: 36.83177947998047
Loss: 36.66828536987305
Loss: 36.65812683105469
Loss: 36.643951416015625
Loss: 36.662689208984375
Loss: 36.71432113647461
Loss: 36.66460418701172
Loss: 36.63429260253906
Loss: 36.604286193847656
Loss: 36.616329193115234
[Train] Epoch 6, accuracy 0.012760416666666666
[Eval] Epoch 6, accuracy 0.01597222222222222
Model saved as model_weights_best.pth
Loss: 36.729183197021484
Loss: 36.6534423828125
Loss: 36.5038948059082
Loss: 36.690330505371094
Loss: 36.49064636230469
Loss: 36.37556838989258
Loss: 36.63970184326172
Loss: 36.705238342285156
Loss: 36.58527755737305
Loss: 36.703460693359375
Loss: 36.48832702636719
Loss: 36.65944290161133
Loss: 36.54042053222656
Loss: 36.55851364135742
Loss: 36.7098388671875
Loss: 36.5140495300293
Loss: 36.523563385009766
Loss: 36.56934356689453
Loss: 36.51866912841797
Loss: 36.62982940673828
Loss: 36.6904296875
Loss: 36.653053283691406
Loss: 36.5889778137207
Loss: 36.61293411254883
Loss: 36.67232894897461
Loss: 36.57646179199219
Loss: 36.5075569152832
Loss: 36.56587600708008
Loss: 36.47685241699219
Loss: 36.695228576660156
Loss: 36.4404411315918
Loss: 36.457706451416016
Loss: 36.498756408691406
Loss: 36.53490447998047
Loss: 36.52302551269531
Loss: 36.53340148925781
Loss: 36.62567901611328
Loss: 36.55989456176758
Loss: 36.47944641113281
Loss: 36.358848571777344
Loss: 36.37235641479492
Loss: 36.55221176147461
Loss: 36.44025421142578
Loss: 36.51727294921875
Loss: 36.538299560546875
[Train] Epoch 7, accuracy 0.021527777777777778
[Eval] Epoch 7, accuracy 0.029166666666666667
Model saved as model_weights_best.pth
Loss: 36.338539123535156
Loss: 36.535301208496094
Loss: 36.45164108276367
Loss: 36.35850524902344
Loss: 36.40994644165039
Loss: 36.4446907043457
Loss: 36.37618637084961
Loss: 36.53499984741211
Loss: 36.398860931396484
Loss: 36.54214096069336
Loss: 36.32368469238281
Loss: 36.548622131347656
Loss: 36.49547576904297
Loss: 36.37054443359375
Loss: 36.46721267700195
Loss: 36.45199203491211
Loss: 36.27210998535156
Loss: 36.338497161865234
Loss: 36.324275970458984
Loss: 36.30494689941406
Loss: 36.579158782958984
Loss: 36.3497428894043
Loss: 36.474239349365234
Loss: 36.33329391479492
Loss: 36.267574310302734
Loss: 36.309288024902344
Loss: 36.37556838989258
Loss: 36.347537994384766
Loss: 36.28709030151367
Loss: 36.19190216064453
Loss: 36.33641815185547
Loss: 36.11724090576172
Loss: 36.290687561035156
Loss: 36.17138671875
Loss: 36.225318908691406
Loss: 36.33085632324219
Loss: 36.273590087890625
Loss: 36.20613098144531
Loss: 36.24762725830078
Loss: 36.33096694946289
Loss: 36.38764953613281
Loss: 36.239376068115234
Loss: 36.179595947265625
Loss: 36.224830627441406
Loss: 36.29526138305664
[Train] Epoch 8, accuracy 0.03550347222222222
[Eval] Epoch 8, accuracy 0.04548611111111111
Model saved as model_weights_best.pth
Loss: 36.125770568847656
Loss: 36.141868591308594
Loss: 36.182979583740234
Loss: 36.21928405761719
Loss: 36.11671829223633
Loss: 36.21641159057617
Loss: 35.98823165893555
Loss: 36.10859298706055
Loss: 36.16362762451172
Loss: 36.24289321899414
Loss: 36.202632904052734
Loss: 36.06001663208008
Loss: 36.23582077026367
Loss: 36.09867477416992
Loss: 36.02608871459961
Loss: 36.028480529785156
Loss: 36.08325958251953
Loss: 36.155418395996094
Loss: 36.311588287353516
Loss: 36.11077880859375
Loss: 36.016258239746094
Loss: 36.13374328613281
Loss: 35.9257698059082
Loss: 36.19245910644531
Loss: 36.11988830566406
Loss: 35.94698715209961
Loss: 36.046348571777344
Loss: 35.97241973876953
Loss: 36.05443572998047
Loss: 36.022544860839844
Loss: 35.947635650634766
Loss: 36.054073333740234
Loss: 35.90745544433594
Loss: 36.014488220214844
Loss: 35.86526107788086
Loss: 35.79142379760742
Loss: 35.87399673461914
Loss: 35.98215866088867
Loss: 35.9564323425293
Loss: 36.045204162597656
Loss: 35.96516418457031
Loss: 36.065040588378906
Loss: 36.1325798034668
Loss: 35.661888122558594
Loss: 35.69504928588867
[Train] Epoch 9, accuracy 0.06180555555555556
[Eval] Epoch 9, accuracy 0.07395833333333333
Model saved as model_weights_best.pth
Loss: 35.76734924316406
Loss: 35.83771514892578
Loss: 35.85682678222656
Loss: 35.69465637207031
Loss: 35.67251205444336
Loss: 35.99006652832031
Loss: 35.665283203125
Loss: 35.75507354736328
Loss: 35.59583282470703
Loss: 35.66348648071289
Loss: 35.56917953491211
Loss: 35.69034957885742
Loss: 35.82234191894531
Loss: 35.77136993408203
Loss: 35.50225067138672
Loss: 35.42005920410156
Loss: 35.633750915527344
Loss: 35.73387145996094
Loss: 35.660404205322266
Loss: 35.585575103759766
Loss: 35.51792907714844
Loss: 35.611602783203125
Loss: 35.671321868896484
Loss: 35.50228500366211
Loss: 35.418182373046875
Loss: 35.570865631103516
Loss: 35.58202362060547
Loss: 35.52553939819336
Loss: 35.653011322021484
Loss: 35.529605865478516
Loss: 35.321529388427734
Loss: 35.51050567626953
Loss: 35.21479034423828
Loss: 35.460567474365234
Loss: 35.34469223022461
Loss: 35.48119354248047
Loss: 35.427268981933594
Loss: 35.47787857055664
Loss: 35.32014846801758
Loss: 35.38489532470703
Loss: 35.37552261352539
Loss: 35.27943420410156
Loss: 35.40330505371094
Loss: 35.215789794921875
Loss: 35.16655349731445
[Train] Epoch 10, accuracy 0.11675347222222222
[Eval] Epoch 10, accuracy 0.15868055555555555
Model saved as model_weights_best.pth
Loss: 35.23478317260742
Loss: 35.13043975830078
Loss: 35.104209899902344
Loss: 35.04246520996094
Loss: 35.172359466552734
Loss: 35.145790100097656
Loss: 35.15849685668945
Loss: 34.8985595703125
Loss: 34.85609436035156
Loss: 34.97093963623047
Loss: 35.05784606933594
Loss: 34.94995880126953
Loss: 35.00577163696289
Loss: 34.98068618774414
Loss: 34.911441802978516
Loss: 35.02365493774414
Loss: 34.998538970947266
Loss: 34.73897933959961
Loss: 35.00133514404297
Loss: 34.791500091552734
Loss: 34.60225296020508
Loss: 34.8202018737793
Loss: 34.80412673950195
Loss: 34.754981994628906
Loss: 34.70646667480469
Loss: 34.7556266784668
Loss: 34.74759292602539
Loss: 34.7297248840332
Loss: 34.49356460571289
Loss: 34.597347259521484
Loss: 34.504825592041016
Loss: 34.306678771972656
Loss: 34.47737121582031
Loss: 34.39188003540039
Loss: 34.432861328125
Loss: 34.47109603881836
Loss: 34.558372497558594
Loss: 34.590049743652344
Loss: 34.446773529052734
Loss: 34.179893493652344
Loss: 34.32232666015625
Loss: 34.54328918457031
Loss: 34.12165832519531
Loss: 34.20397186279297
Loss: 34.17177200317383
[Train] Epoch 11, accuracy 0.22934027777777777
[Eval] Epoch 11, accuracy 0.2708333333333333
Model saved as model_weights_best.pth
Loss: 33.879356384277344
Loss: 33.989173889160156
Loss: 34.24322509765625
Loss: 33.94684982299805
Loss: 33.99457550048828
Loss: 33.898460388183594
Loss: 33.826297760009766
Loss: 34.019927978515625
Loss: 33.91079330444336
Loss: 33.91523742675781
Loss: 33.787437438964844
Loss: 33.85835647583008
Loss: 33.613731384277344
Loss: 33.79191207885742
Loss: 34.09981918334961
Loss: 33.60645294189453
Loss: 33.65157699584961
Loss: 33.848812103271484
Loss: 33.75647735595703
Loss: 33.685848236083984
Loss: 33.75080490112305
Loss: 33.855960845947266
Loss: 33.643516540527344
Loss: 33.79887771606445
Loss: 33.800540924072266
Loss: 33.666107177734375
Loss: 33.64259719848633
Loss: 33.41642761230469
Loss: 33.854522705078125
Loss: 33.41368865966797
Loss: 33.33527755737305
Loss: 33.398681640625
Loss: 33.26591873168945
Loss: 33.367942810058594
Loss: 33.06311798095703
Loss: 33.38692092895508
Loss: 33.439796447753906
Loss: 33.29893493652344
Loss: 33.30073928833008
Loss: 33.111209869384766
Loss: 33.447383880615234
Loss: 33.49860763549805
Loss: 33.1582145690918
Loss: 33.387847900390625
Loss: 33.1960563659668
[Train] Epoch 12, accuracy 0.38289930555555557
[Eval] Epoch 12, accuracy 0.39479166666666665
Model saved as model_weights_best.pth
Loss: 32.59547424316406
Loss: 32.88282775878906
Loss: 32.85469055175781
Loss: 32.839332580566406
Loss: 32.659027099609375
Loss: 32.80602264404297
Loss: 32.5699348449707
Loss: 33.0449333190918
Loss: 32.75909423828125
Loss: 32.598716735839844
Loss: 33.01694869995117
Loss: 32.58549118041992
Loss: 32.60039138793945
Loss: 32.64103317260742
Loss: 32.6584587097168
Loss: 32.307037353515625
Loss: 32.84992980957031
Loss: 32.5014533996582
Loss: 32.567684173583984
Loss: 32.63280487060547
Loss: 32.51679611206055
Loss: 32.567222595214844
Loss: 32.22833251953125
Loss: 32.668243408203125
Loss: 32.780216217041016
Loss: 31.803800582885742
Loss: 32.35269546508789
Loss: 32.323890686035156
Loss: 32.515106201171875
Loss: 32.48511505126953
Loss: 31.872194290161133
Loss: 32.32615280151367
Loss: 32.20012283325195
Loss: 32.2630500793457
Loss: 32.01809310913086
Loss: 32.10682678222656
Loss: 32.504329681396484
Loss: 31.722225189208984
Loss: 32.11587905883789
Loss: 32.41522216796875
Loss: 31.782634735107422
Loss: 31.651187896728516
Loss: 32.2556266784668
Loss: 31.901691436767578
Loss: 32.340065002441406
[Train] Epoch 13, accuracy 0.5461805555555556
[Eval] Epoch 13, accuracy 0.5513888888888889
Model saved as model_weights_best.pth
Loss: 31.403316497802734
Loss: 31.86222267150879
Loss: 31.660783767700195
Loss: 31.886323928833008
Loss: 31.323278427124023
Loss: 31.20380401611328
Loss: 31.24782943725586
Loss: 31.637365341186523
Loss: 31.558256149291992
Loss: 31.789182662963867
Loss: 31.42575454711914
Loss: 31.383644104003906
Loss: 31.37618064880371
Loss: 31.3763370513916
Loss: 31.327346801757812
Loss: 31.19136619567871
Loss: 31.145326614379883
Loss: 31.009098052978516
Loss: 31.274717330932617
Loss: 30.76645278930664
Loss: 31.361858367919922
Loss: 31.395477294921875
Loss: 31.009815216064453
Loss: 31.220205307006836
Loss: 31.350997924804688
Loss: 31.6846923828125
Loss: 31.227073669433594
Loss: 31.19057273864746
Loss: 31.19097137451172
Loss: 30.445362091064453
Loss: 31.19080352783203
Loss: 30.633195877075195
Loss: 30.729183197021484
Loss: 30.570911407470703
Loss: 30.873046875
Loss: 30.9791259765625
Loss: 31.04708480834961
Loss: 30.823833465576172
Loss: 30.66300392150879
Loss: 30.324512481689453
Loss: 30.576702117919922
Loss: 30.356273651123047
Loss: 30.51677131652832
Loss: 29.784175872802734
Loss: 30.775297164916992
[Train] Epoch 14, accuracy 0.6827256944444444
[Eval] Epoch 14, accuracy 0.6270833333333333
Model saved as model_weights_best.pth
Loss: 30.34532356262207
Loss: 30.13962173461914
Loss: 30.463035583496094
Loss: 30.2882080078125
Loss: 29.589372634887695
Loss: 30.365589141845703
Loss: 29.907899856567383
Loss: 30.086421966552734
Loss: 30.130144119262695
Loss: 30.017696380615234
Loss: 30.09604263305664
Loss: 29.972841262817383
Loss: 29.874961853027344
Loss: 29.9237117767334
Loss: 30.357269287109375
Loss: 29.43077850341797
Loss: 29.769515991210938
Loss: 30.05276870727539
Loss: 30.1413631439209
Loss: 29.823932647705078
Loss: 29.673603057861328
Loss: 29.657329559326172
Loss: 30.026233673095703
Loss: 29.50360107421875
Loss: 29.525938034057617
Loss: 29.415037155151367
Loss: 29.773998260498047
Loss: 28.906795501708984
Loss: 29.23689079284668
Loss: 29.298978805541992
Loss: 29.467737197875977
Loss: 28.937889099121094
Loss: 29.19280242919922
Loss: 28.86229705810547
Loss: 29.262723922729492
Loss: 29.26022720336914
Loss: 29.22461700439453
Loss: 29.177946090698242
Loss: 28.768857955932617
Loss: 29.060522079467773
Loss: 29.451080322265625
Loss: 28.786392211914062
Loss: 29.191993713378906
Loss: 28.917688369750977
Loss: 28.863330841064453
[Train] Epoch 15, accuracy 0.7768229166666667
[Eval] Epoch 15, accuracy 0.7284722222222222
Model saved as model_weights_best.pth
Loss: 28.81304931640625
Loss: 28.66038703918457
Loss: 28.218914031982422
Loss: 28.366830825805664
Loss: 28.527782440185547
Loss: 29.060874938964844
Loss: 28.208330154418945
Loss: 28.345130920410156
Loss: 28.339967727661133
Loss: 27.90376091003418
Loss: 28.17496109008789
Loss: 28.36585235595703
Loss: 27.69328498840332
Loss: 28.153379440307617
Loss: 28.438274383544922
Loss: 27.8139705657959
Loss: 27.777854919433594
Loss: 27.93620491027832
Loss: 27.84918975830078
Loss: 28.093748092651367
Loss: 27.723878860473633
Loss: 27.545616149902344
Loss: 27.986520767211914
Loss: 27.754262924194336
Loss: 27.687957763671875
Loss: 27.507354736328125
Loss: 26.991472244262695
Loss: 28.26974868774414
Loss: 27.747671127319336
Loss: 27.72681999206543
Loss: 27.29621696472168
Loss: 27.161359786987305
Loss: 27.651309967041016
Loss: 26.918527603149414
Loss: 27.622953414916992
Loss: 26.932573318481445
Loss: 26.805835723876953
Loss: 26.582820892333984
Loss: 26.644493103027344
Loss: 27.459211349487305
Loss: 27.513931274414062
Loss: 26.40851593017578
Loss: 27.06214141845703
Loss: 26.704599380493164
Loss: 27.188175201416016
[Train] Epoch 16, accuracy 0.8426215277777778
[Eval] Epoch 16, accuracy 0.7934027777777778
Model saved as model_weights_best.pth
Loss: 25.705883026123047
Loss: 26.75179672241211
Loss: 26.32925033569336
Loss: 26.315479278564453
Loss: 26.200420379638672
Loss: 26.315034866333008
Loss: 25.875938415527344
Loss: 26.38796043395996
Loss: 25.97003746032715
Loss: 26.913434982299805
Loss: 25.917224884033203
Loss: 25.459487915039062
Loss: 27.214860916137695
Loss: 25.962961196899414
Loss: 26.124786376953125
Loss: 25.735759735107422
Loss: 26.345232009887695
Loss: 25.709842681884766
Loss: 26.572744369506836
Loss: 25.760242462158203
Loss: 25.78465461730957
Loss: 26.149118423461914
Loss: 25.440731048583984
Loss: 25.857275009155273
Loss: 26.205608367919922
Loss: 25.200153350830078
Loss: 25.51264762878418
Loss: 26.213809967041016
Loss: 25.65374183654785
Loss: 25.379301071166992
Loss: 25.580455780029297
Loss: 25.351856231689453
Loss: 25.262182235717773
Loss: 24.728883743286133
Loss: 25.431440353393555
Loss: 25.507368087768555
Loss: 25.388916015625
Loss: 25.146099090576172
Loss: 24.201478958129883
Loss: 24.470762252807617
Loss: 24.37298583984375
Loss: 24.663850784301758
Loss: 24.247697830200195
Loss: 25.14493179321289
Loss: 24.1734676361084
[Train] Epoch 17, accuracy 0.8784722222222222
[Eval] Epoch 17, accuracy 0.8475694444444445
Model saved as model_weights_best.pth
Loss: 23.933053970336914
Loss: 24.886362075805664
Loss: 24.42707061767578
Loss: 24.104162216186523
Loss: 24.481779098510742
Loss: 24.01303482055664
Loss: 23.866451263427734
Loss: 23.848844528198242
Loss: 24.336320877075195
Loss: 23.421127319335938
Loss: 24.146324157714844
Loss: 24.916736602783203
Loss: 24.267932891845703
Loss: 24.50659942626953
Loss: 24.67388153076172
Loss: 23.443016052246094
Loss: 23.585861206054688
Loss: 23.8632755279541
Loss: 24.052766799926758
Loss: 23.206510543823242
Loss: 23.686744689941406
Loss: 23.127729415893555
Loss: 23.6132755279541
Loss: 23.249893188476562
Loss: 23.086198806762695
Loss: 23.136693954467773
Loss: 22.674894332885742
Loss: 23.17635726928711
Loss: 22.616975784301758
Loss: 22.800138473510742
Loss: 23.307159423828125
Loss: 22.554243087768555
Loss: 22.497560501098633
Loss: 23.046241760253906
Loss: 22.84880256652832
Loss: 22.603824615478516
Loss: 21.657636642456055
Loss: 22.086278915405273
Loss: 22.50776481628418
Loss: 21.981395721435547
Loss: 21.201465606689453
Loss: 22.157201766967773
Loss: 22.299840927124023
Loss: 21.28849220275879
Loss: 21.94628143310547
[Train] Epoch 18, accuracy 0.9150173611111111
[Eval] Epoch 18, accuracy 0.8940972222222222
Model saved as model_weights_best.pth
Loss: 21.851083755493164
Loss: 21.05048179626465
Loss: 20.45335578918457
Loss: 21.20792579650879
Loss: 21.208099365234375
Loss: 21.304643630981445
Loss: 20.409608840942383
Loss: 21.022911071777344
Loss: 21.35063934326172
Loss: 20.940807342529297
Loss: 19.773897171020508
Loss: 20.61965560913086
Loss: 21.766878128051758
Loss: 19.61770248413086
Loss: 24.126548767089844
Loss: 20.525741577148438
Loss: 20.626367568969727
Loss: 21.5090274810791
Loss: 21.017765045166016
Loss: 21.39808464050293
Loss: 20.60894203186035
Loss: 20.452678680419922
Loss: 21.383438110351562
Loss: 20.946685791015625
Loss: 21.08049201965332
Loss: 20.62199592590332
Loss: 21.414119720458984
Loss: 21.301528930664062
Loss: 20.333498001098633
Loss: 20.38884162902832
Loss: 20.577165603637695
Loss: 19.67120933532715
Loss: 20.822734832763672
Loss: 20.259382247924805
Loss: 20.373214721679688
Loss: 19.845365524291992
Loss: 20.020795822143555
Loss: 20.453418731689453
Loss: 20.40935707092285
Loss: 20.561656951904297
Loss: 19.793039321899414
Loss: 19.723024368286133
Loss: 20.42884635925293
Loss: 20.066247940063477
Loss: 20.21876335144043
[Train] Epoch 19, accuracy 0.9375
[Eval] Epoch 19, accuracy 0.9260416666666667
Model saved as model_weights_best.pth
Loss: 18.583696365356445
Loss: 19.934385299682617
Loss: 19.108678817749023
Loss: 18.629056930541992
Loss: 18.66729736328125
Loss: 18.85094451904297
Loss: 19.31078338623047
Loss: 18.24896240234375
Loss: 18.1228084564209
Loss: 18.085824966430664
Loss: 18.610668182373047
Loss: 18.351638793945312
Loss: 18.502098083496094
Loss: 19.230669021606445
Loss: 17.97427749633789
Loss: 18.5701961517334
Loss: 18.13627815246582
Loss: 18.242006301879883
Loss: 17.411941528320312
Loss: 18.209228515625
Loss: 17.894546508789062
Loss: 17.70513343811035
Loss: 18.214332580566406
Loss: 18.06561279296875
Loss: 17.11538314819336
Loss: 17.79920768737793
Loss: 17.366622924804688
Loss: 17.563108444213867
Loss: 17.488840103149414
Loss: 18.16301918029785
Loss: 18.032699584960938
Loss: 17.059885025024414
Loss: 17.357534408569336
Loss: 18.2281436920166
Loss: 18.578826904296875
Loss: 17.882036209106445
Loss: 18.000823974609375
Loss: 16.841079711914062
Loss: 17.034353256225586
Loss: 16.96331787109375
Loss: 17.29900550842285
Loss: 16.635068893432617
Loss: 16.0937557220459
Loss: 17.464202880859375
Loss: 16.734865188598633
[Train] Epoch 20, accuracy 0.9574652777777778
[Eval] Epoch 20, accuracy 0.9458333333333333
Model saved as model_weights_best.pth
Loss: 15.84892463684082
Loss: 16.901123046875
Loss: 16.320497512817383
Loss: 16.327484130859375
Loss: 16.307331085205078
Loss: 16.685239791870117
Loss: 16.82772445678711
Loss: 16.679378509521484
Loss: 15.473546028137207
Loss: 15.317398071289062
Loss: 15.842395782470703
Loss: 16.26193618774414
Loss: 16.03032875061035
Loss: 15.974211692810059
Loss: 15.841085433959961
Loss: 15.873050689697266
Loss: 15.447502136230469
Loss: 15.849187850952148
Loss: 15.435524940490723
Loss: 16.57785987854004
Loss: 16.44646644592285
Loss: 15.69100284576416
Loss: 15.305397987365723
Loss: 16.282846450805664
Loss: 15.172489166259766
Loss: 15.113085746765137
Loss: 16.375219345092773
Loss: 14.905450820922852
Loss: 15.231461524963379
Loss: 15.277632713317871
Loss: 14.846004486083984
Loss: 15.982770919799805
Loss: 15.853738784790039
Loss: 15.152599334716797
Loss: 14.537775039672852
Loss: 15.014765739440918
Loss: 14.810528755187988
Loss: 16.193748474121094
Loss: 15.174832344055176
Loss: 14.995681762695312
Loss: 14.040741920471191
Loss: 14.78507137298584
Loss: 15.666744232177734
Loss: 14.201316833496094
Loss: 15.240168571472168
[Train] Epoch 21, accuracy 0.9682291666666667
[Eval] Epoch 21, accuracy 0.9572916666666667
Model saved as model_weights_best.pth
Loss: 13.578502655029297
Loss: 14.039787292480469
Loss: 14.468405723571777
Loss: 13.329809188842773
Loss: 13.537409782409668
Loss: 14.021357536315918
Loss: 13.496148109436035
Loss: 14.230849266052246
Loss: 13.828153610229492
Loss: 13.611589431762695
Loss: 15.042339324951172
Loss: 14.256049156188965
Loss: 13.544602394104004
Loss: 14.017510414123535
Loss: 13.51411247253418
Loss: 13.62436294555664
Loss: 13.645270347595215
Loss: 13.902078628540039
Loss: 13.314319610595703
Loss: 13.328900337219238
Loss: 13.619413375854492
Loss: 13.376974105834961
Loss: 13.826554298400879
Loss: 13.47289752960205
Loss: 12.415664672851562
Loss: 13.565725326538086
Loss: 13.113906860351562
Loss: 12.61954116821289
Loss: 12.39698314666748
Loss: 13.52700138092041
Loss: 12.967007637023926
Loss: 13.2606201171875
Loss: 13.726386070251465
Loss: 13.327605247497559
Loss: 12.713615417480469
Loss: 12.73010540008545
Loss: 13.112480163574219
Loss: 12.516778945922852
Loss: 13.347360610961914
Loss: 12.606721878051758
Loss: 12.2268705368042
Loss: 12.71950626373291
Loss: 12.222386360168457
Loss: 13.352349281311035
Loss: 13.335882186889648
[Train] Epoch 22, accuracy 0.97578125
[Eval] Epoch 22, accuracy 0.9701388888888889
Model saved as model_weights_best.pth
Loss: 12.522904396057129
Loss: 12.012527465820312
Loss: 12.760233879089355
Loss: 12.397420883178711
Loss: 11.778071403503418
Loss: 11.579830169677734
Loss: 12.220892906188965
Loss: 12.037155151367188
Loss: 11.960976600646973
Loss: 11.7298002243042
Loss: 12.490066528320312
Loss: 11.695525169372559
Loss: 12.167665481567383
Loss: 10.977381706237793
Loss: 11.670092582702637
Loss: 12.532561302185059
Loss: 11.689985275268555
Loss: 12.479851722717285
Loss: 12.626741409301758
Loss: 10.735182762145996
Loss: 12.027986526489258
Loss: 11.740198135375977
Loss: 11.588325500488281
Loss: 11.129180908203125
Loss: 11.15688419342041
Loss: 11.625900268554688
Loss: 11.949846267700195
Loss: 10.658540725708008
Loss: 11.629810333251953
Loss: 11.938372611999512
Loss: 10.779691696166992
Loss: 11.907638549804688
Loss: 11.5880126953125
Loss: 11.747685432434082
Loss: 10.81434154510498
Loss: 10.933414459228516
Loss: 10.858091354370117
Loss: 11.99044418334961
Loss: 10.635660171508789
Loss: 9.857102394104004
Loss: 11.610077857971191
Loss: 11.09632396697998
Loss: 10.912029266357422
Loss: 11.569574356079102
Loss: 10.436971664428711
[Train] Epoch 23, accuracy 0.9805555555555555
[Eval] Epoch 23, accuracy 0.9760416666666667
Model saved as model_weights_best.pth
Loss: 10.086647033691406
Loss: 9.790270805358887
Loss: 11.31051254272461
Loss: 10.287186622619629
Loss: 9.985358238220215
Loss: 10.449230194091797
Loss: 10.608931541442871
Loss: 10.542560577392578
Loss: 10.516860008239746
Loss: 10.340156555175781
Loss: 9.721197128295898
Loss: 11.018446922302246
Loss: 9.848031997680664
Loss: 10.5541410446167
Loss: 9.812506675720215
Loss: 10.511387825012207
Loss: 9.192824363708496
Loss: 9.644283294677734
Loss: 10.269733428955078
Loss: 9.716678619384766
Loss: 9.461536407470703
Loss: 10.741874694824219
Loss: 9.104223251342773
Loss: 10.54473876953125
Loss: 9.974287033081055
Loss: 10.508127212524414
Loss: 11.149931907653809
Loss: 10.352203369140625
Loss: 9.647012710571289
Loss: 9.486576080322266
Loss: 9.686389923095703
Loss: 9.727808952331543
Loss: 9.94823169708252
Loss: 9.54958724975586
Loss: 9.489999771118164
Loss: 9.47870922088623
Loss: 9.82744312286377
Loss: 9.565895080566406
Loss: 9.23493480682373
Loss: 10.069376945495605
Loss: 10.066858291625977
Loss: 9.873591423034668
Loss: 10.204293251037598
Loss: 9.074413299560547
Loss: 9.859992980957031
[Train] Epoch 24, accuracy 0.9854166666666667
[Eval] Epoch 24, accuracy 0.9677083333333333
Loss: 9.084501266479492
Loss: 9.109837532043457
Loss: 9.035083770751953
Loss: 9.454771041870117
Loss: 10.108003616333008
Loss: 9.561367988586426
Loss: 8.15773868560791
Loss: 9.366220474243164
Loss: 9.191479682922363
Loss: 9.081432342529297
Loss: 9.831144332885742
Loss: 8.826408386230469
Loss: 8.775989532470703
Loss: 9.048250198364258
Loss: 8.999895095825195
Loss: 8.504009246826172
Loss: 9.075408935546875
Loss: 9.42631721496582
Loss: 9.148345947265625
Loss: 8.83234691619873
Loss: 9.19722843170166
Loss: 8.73019790649414
Loss: 8.140602111816406
Loss: 9.047595024108887
Loss: 9.39681339263916
Loss: 8.411835670471191
Loss: 8.646743774414062
Loss: 8.074115753173828
Loss: 8.434670448303223
Loss: 7.919776916503906
Loss: 9.95246410369873
Loss: 8.017500877380371
Loss: 8.808597564697266
Loss: 8.77069091796875
Loss: 8.211387634277344
Loss: 8.390104293823242
Loss: 8.702376365661621
Loss: 9.13983154296875
Loss: 8.269086837768555
Loss: 9.325362205505371
Loss: 9.888232231140137
Loss: 8.31933307647705
Loss: 8.656316757202148
Loss: 7.853606224060059
Loss: 8.34424877166748
[Train] Epoch 25, accuracy 0.9855902777777777
[Eval] Epoch 25, accuracy 0.9826388888888888
Model saved as model_weights_best.pth
Loss: 8.887192726135254
Loss: 7.927559852600098
Loss: 8.414426803588867
Loss: 7.798763275146484
Loss: 8.015573501586914
Loss: 8.105194091796875
Loss: 7.900432586669922
Loss: 7.052909851074219
Loss: 7.905517578125
Loss: 7.506574630737305
Loss: 7.652034759521484
Loss: 7.83158540725708
Loss: 7.176957130432129
Loss: 7.618398189544678
Loss: 7.235990047454834
Loss: 7.285058498382568
Loss: 8.786312103271484
Loss: 7.756052017211914
Loss: 8.604730606079102
Loss: 8.061793327331543
Loss: 7.628109455108643
Loss: 7.778145790100098
Loss: 8.25634479522705
Loss: 6.872539520263672
Loss: 7.957367897033691
Loss: 7.164609909057617
Loss: 7.0456438064575195
Loss: 7.5980448722839355
Loss: 7.783556938171387
Loss: 7.044570446014404
Loss: 6.7004170417785645
Loss: 8.310565948486328
Loss: 6.890257835388184
Loss: 7.1526947021484375
Loss: 7.0920305252075195
Loss: 7.136693954467773
Loss: 6.6984663009643555
Loss: 7.9982523918151855
Loss: 7.890839576721191
Loss: 5.984792232513428
Loss: 7.252123832702637
Loss: 6.7199907302856445
Loss: 7.572913646697998
Loss: 7.142492294311523
Loss: 6.937858581542969
[Train] Epoch 26, accuracy 0.9885416666666667
[Eval] Epoch 26, accuracy 0.9836805555555556
Model saved as model_weights_best.pth
Loss: 6.711419582366943
Loss: 7.764108657836914
Loss: 7.050707817077637
Loss: 5.7791008949279785
Loss: 6.552807807922363
Loss: 6.332147598266602
Loss: 6.808263301849365
Loss: 7.142557621002197
Loss: 6.638657093048096
Loss: 6.195216178894043
Loss: 6.7136664390563965
Loss: 6.925698757171631
Loss: 6.201211452484131
Loss: 6.892518997192383
Loss: 6.817931652069092
Loss: 7.175412178039551
Loss: 7.3773956298828125
Loss: 6.4794416427612305
Loss: 6.802208423614502
Loss: 6.319005966186523
Loss: 6.66910457611084
Loss: 6.26786470413208
Loss: 6.841814994812012
Loss: 6.624039173126221
Loss: 6.80579137802124
Loss: 6.587131500244141
Loss: 6.615691661834717
Loss: 6.857084274291992
Loss: 6.8320136070251465
Loss: 5.3897013664245605
Loss: 7.526041030883789
Loss: 6.638205528259277
Loss: 7.418464183807373
Loss: 6.693175792694092
Loss: 6.241330623626709
Loss: 7.3403730392456055
Loss: 6.855294227600098
Loss: 5.979353904724121
Loss: 6.192733287811279
Loss: 5.9653472900390625
Loss: 6.754772186279297
Loss: 6.547645092010498
Loss: 6.96671199798584
Loss: 6.784569263458252
Loss: 6.7565789222717285
[Train] Epoch 27, accuracy 0.9901909722222222
[Eval] Epoch 27, accuracy 0.9854166666666667
Model saved as model_weights_best.pth
Loss: 6.484691143035889
Loss: 5.750482082366943
Loss: 6.724089622497559
Loss: 5.41949987411499
Loss: 6.5269341468811035
Loss: 6.5245842933654785
Loss: 6.7034382820129395
Loss: 5.9833598136901855
Loss: 6.019678115844727
Loss: 6.597039222717285
Loss: 6.011402130126953
Loss: 6.381749629974365
Loss: 5.5114545822143555
Loss: 6.144720077514648
Loss: 6.409188270568848
Loss: 6.238994121551514
Loss: 6.430504322052002
Loss: 6.1981916427612305
Loss: 5.527519226074219
Loss: 5.658109664916992
Loss: 6.195085048675537
Loss: 6.530332088470459
Loss: 5.78788948059082
Loss: 6.27481746673584
Loss: 6.661672115325928
Loss: 5.253988742828369
Loss: 6.221027851104736
Loss: 5.917389392852783
Loss: 5.338045597076416
Loss: 6.485658645629883
Loss: 5.639710903167725
Loss: 5.396262168884277
Loss: 6.113489151000977
Loss: 5.876069068908691
Loss: 6.078311920166016
Loss: 6.681670665740967
Loss: 6.288914680480957
Loss: 6.14696741104126
Loss: 5.849641799926758
Loss: 5.5984015464782715
Loss: 6.06412410736084
Loss: 5.982536792755127
Loss: 5.948706150054932
Loss: 5.307023525238037
Loss: 5.447371006011963
[Train] Epoch 28, accuracy 0.9905381944444445
[Eval] Epoch 28, accuracy 0.9864583333333333
Model saved as model_weights_best.pth
Loss: 5.775468349456787
Loss: 5.5499267578125
Loss: 5.329311370849609
Loss: 6.941414833068848
Loss: 7.8517584800720215
Loss: 5.571366310119629
Loss: 5.325713157653809
Loss: 5.317011833190918
Loss: 5.593555927276611
Loss: 6.065409183502197
Loss: 6.393741130828857
Loss: 6.609810829162598
Loss: 6.389765739440918
Loss: 5.968494415283203
Loss: 5.800929546356201
Loss: 6.131168842315674
Loss: 6.306273937225342
Loss: 5.688405513763428
Loss: 5.912088394165039
Loss: 5.1947479248046875
Loss: 5.3836894035339355
Loss: 6.046968936920166
Loss: 5.809021949768066
Loss: 5.598304271697998
Loss: 5.231396675109863
Loss: 5.580607891082764
Loss: 6.455757141113281
Loss: 5.376222610473633
Loss: 5.396628379821777
Loss: 5.632155895233154
Loss: 5.778121471405029
Loss: 5.51134729385376
Loss: 5.148219585418701
Loss: 5.261044502258301
Loss: 5.909959316253662
Loss: 5.325379371643066
Loss: 7.352319717407227
Loss: 5.240153789520264
Loss: 5.910751819610596
Loss: 4.591972827911377
Loss: 4.6704583168029785
Loss: 5.214534759521484
Loss: 5.519822120666504
Loss: 5.108612060546875
Loss: 5.630541801452637
[Train] Epoch 29, accuracy 0.9897569444444444
[Eval] Epoch 29, accuracy 0.9836805555555556
Loss: 5.45200777053833
Loss: 4.912842750549316
Loss: 4.832308769226074
Loss: 5.113365173339844
Loss: 4.774759292602539
Loss: 5.164191722869873
Loss: 5.32697057723999
Loss: 4.662503719329834
Loss: 4.732354164123535
Loss: 4.587374687194824
Loss: 4.692246437072754
Loss: 4.42190408706665
Loss: 4.439858913421631
Loss: 4.829315662384033
Loss: 5.008748531341553
Loss: 4.412793159484863
Loss: 5.290582656860352
Loss: 5.148489952087402
Loss: 4.986909866333008
Loss: 5.243419647216797
Loss: 4.698419094085693
Loss: 5.051492214202881
Loss: 4.89298677444458
Loss: 4.86543607711792
Loss: 5.931254863739014
Loss: 4.391795635223389
Loss: 5.6185832023620605
Loss: 5.411367416381836
Loss: 4.861220836639404
Loss: 5.18790340423584
Loss: 5.197174072265625
Loss: 4.983601093292236
Loss: 4.8401641845703125
Loss: 6.274957656860352
Loss: 4.876099109649658
Loss: 4.256715297698975
Loss: 4.958529949188232
Loss: 4.510436058044434
Loss: 4.51289701461792
Loss: 4.977269172668457
Loss: 5.321093559265137
Loss: 4.556748390197754
Loss: 4.792953014373779
Loss: 5.581569194793701
Loss: 5.3407979011535645
[Train] Epoch 30, accuracy 0.9907118055555556
[Eval] Epoch 30, accuracy 0.9864583333333333
Loss: 5.258300304412842
Loss: 4.412073612213135
Loss: 4.806373119354248
Loss: 4.750787258148193
Loss: 5.08658504486084
Loss: 5.045426845550537
Loss: 4.85875129699707
Loss: 5.2033162117004395
Loss: 4.561441421508789
Loss: 4.6468024253845215
Loss: 4.207825183868408
Loss: 4.754690647125244
Loss: 4.777577877044678
Loss: 4.327714443206787
Loss: 4.5584235191345215
Loss: 4.4547505378723145
Loss: 4.049283981323242
Loss: 4.676295757293701
Loss: 5.2445969581604
Loss: 4.2065205574035645
Loss: 4.808403491973877
Loss: 5.197996139526367
Loss: 4.214356422424316
Loss: 4.48307991027832
Loss: 4.4679274559021
Loss: 5.012896537780762
Loss: 4.8466949462890625
Loss: 4.31303596496582
Loss: 4.0965681076049805
Loss: 4.252542018890381
Loss: 4.403162002563477
Loss: 5.0498552322387695
Loss: 5.897183418273926
Loss: 4.549525260925293
Loss: 3.979586124420166
Loss: 4.136316776275635
Loss: 4.447397708892822
Loss: 4.685669422149658
Loss: 4.195352554321289
Loss: 3.871370315551758
Loss: 3.5936248302459717
Loss: 4.978715896606445
Loss: 5.244769096374512
Loss: 4.897925853729248
Loss: 4.062685012817383
[Train] Epoch 31, accuracy 0.9914930555555556
[Eval] Epoch 31, accuracy 0.9857638888888889
Loss: 4.0223307609558105
Loss: 5.277919769287109
Loss: 4.426285266876221
Loss: 3.566409111022949
Loss: 3.8484139442443848
Loss: 4.108499050140381
Loss: 4.792079925537109
Loss: 4.375522613525391
Loss: 4.00466775894165
Loss: 3.7739601135253906
Loss: 4.268953323364258
Loss: 4.802112579345703
Loss: 4.166239261627197
Loss: 3.65873122215271
Loss: 4.396346092224121
Loss: 4.165350914001465
Loss: 4.27337121963501
Loss: 4.167247295379639
Loss: 4.139278888702393
Loss: 3.608549118041992
Loss: 3.885514974594116
Loss: 3.971360683441162
Loss: 3.7455878257751465
Loss: 4.738299369812012
Loss: 4.778444766998291
Loss: 4.719745635986328
Loss: 4.3633294105529785
Loss: 3.7711920738220215
Loss: 3.591630697250366
Loss: 3.788792848587036
Loss: 4.25160551071167
Loss: 4.511151313781738
Loss: 4.478107929229736
Loss: 4.294430255889893
Loss: 3.8129303455352783
Loss: 3.7571706771850586
Loss: 4.743081092834473
Loss: 4.517158508300781
Loss: 4.015137672424316
Loss: 3.3374457359313965
Loss: 4.1195573806762695
Loss: 4.458112716674805
Loss: 3.685060739517212
Loss: 4.643977165222168
Loss: 4.5051069259643555
[Train] Epoch 32, accuracy 0.9917534722222222
[Eval] Epoch 32, accuracy 0.9868055555555556
Model saved as model_weights_best.pth
Loss: 4.0960259437561035
Loss: 3.914470911026001
Loss: 3.9762306213378906
Loss: 3.676193952560425
Loss: 3.7806074619293213
Loss: 4.367760181427002
Loss: 4.30654239654541
Loss: 4.245832920074463
Loss: 4.311184406280518
Loss: 4.227397918701172
Loss: 4.4174041748046875
Loss: 4.390434265136719
Loss: 3.2827517986297607
Loss: 3.569767713546753
Loss: 3.729069232940674
Loss: 3.515882730484009
Loss: 3.7779877185821533
Loss: 3.687462568283081
Loss: 4.325619697570801
Loss: 3.865480899810791
Loss: 3.591137170791626
Loss: 3.557457685470581
Loss: 3.7253408432006836
Loss: 3.470780849456787
Loss: 3.833991527557373
Loss: 4.039029121398926
Loss: 4.5599212646484375
Loss: 3.276963233947754
Loss: 3.790337562561035
Loss: 4.272264003753662
Loss: 4.0651726722717285
Loss: 2.8863346576690674
Loss: 4.039987564086914
Loss: 3.978376865386963
Loss: 3.8531346321105957
Loss: 3.8262901306152344
Loss: 4.054391860961914
Loss: 4.572840690612793
Loss: 3.66729474067688
Loss: 3.3766634464263916
Loss: 3.878570556640625
Loss: 4.264225006103516
Loss: 3.2082271575927734
Loss: 3.9821972846984863
Loss: 4.151245594024658
[Train] Epoch 33, accuracy 0.9934895833333334
[Eval] Epoch 33, accuracy 0.9875
Model saved as model_weights_best.pth
Loss: 3.7065584659576416
Loss: 3.2822139263153076
Loss: 3.965298652648926
Loss: 3.4165921211242676
Loss: 4.2163166999816895
Loss: 3.5709145069122314
Loss: 3.5532538890838623
Loss: 3.732926845550537
Loss: 3.49495267868042
Loss: 3.799607038497925
Loss: 3.7833714485168457
Loss: 3.9213526248931885
Loss: 3.4225823879241943
Loss: 3.6257052421569824
Loss: 3.600449562072754
Loss: 3.6139638423919678
Loss: 4.274392127990723
Loss: 3.6873722076416016
Loss: 3.604335308074951
Loss: 3.8718175888061523
Loss: 4.018391132354736
Loss: 3.1101675033569336
Loss: 3.2876577377319336
Loss: 3.2720820903778076
Loss: 3.448310136795044
Loss: 3.102471351623535
Loss: 3.3378283977508545
Loss: 3.6264073848724365
Loss: 3.7285706996917725
Loss: 3.853402853012085
Loss: 3.6369245052337646
Loss: 4.195921897888184
Loss: 3.290858268737793
Loss: 3.6840217113494873
Loss: 3.525172472000122
Loss: 3.5684518814086914
Loss: 3.5682756900787354
Loss: 4.123168468475342
Loss: 4.38835334777832
Loss: 3.342413902282715
Loss: 3.8303871154785156
Loss: 3.5078094005584717
Loss: 3.721221923828125
Loss: 3.163344621658325
Loss: 3.805903673171997
[Train] Epoch 34, accuracy 0.9935763888888889
[Eval] Epoch 34, accuracy 0.9881944444444445
Model saved as model_weights_best.pth
Loss: 3.2915070056915283
Loss: 3.1379895210266113
Loss: 3.1832449436187744
Loss: 4.724710464477539
Loss: 3.349755048751831
Loss: 3.8346951007843018
Loss: 3.1722307205200195
Loss: 3.549959897994995
Loss: 3.6123969554901123
Loss: 3.007601499557495
Loss: 3.7627358436584473
Loss: 3.3914260864257812
Loss: 3.025984525680542
Loss: 3.295388698577881
Loss: 3.2704782485961914
Loss: 3.9922590255737305
Loss: 3.4405357837677
Loss: 3.8040289878845215
Loss: 3.78460431098938
Loss: 3.923602342605591
Loss: 3.086775541305542
Loss: 3.5155444145202637
Loss: 3.8389339447021484
Loss: 3.545243263244629
Loss: 3.967777729034424
Loss: 3.2816274166107178
Loss: 3.776517152786255
Loss: 3.736623764038086
Loss: 3.1257829666137695
Loss: 3.459822654724121
Loss: 3.8187334537506104
Loss: 3.4898104667663574
Loss: 3.8060343265533447
Loss: 3.835623264312744
Loss: 3.2182681560516357
Loss: 3.3583335876464844
Loss: 3.5850486755371094
Loss: 3.8523142337799072
Loss: 3.872580051422119
Loss: 3.584925889968872
Loss: 3.522974967956543
Loss: 3.7094006538391113
Loss: 3.6787824630737305
Loss: 3.0564918518066406
Loss: 3.4593677520751953
[Train] Epoch 35, accuracy 0.9932291666666667
[Eval] Epoch 35, accuracy 0.9881944444444445
Loss: 3.5443644523620605
Loss: 3.4059596061706543
Loss: 3.4469337463378906
Loss: 3.3431079387664795
Loss: 2.8633365631103516
Loss: 3.0305025577545166
Loss: 3.258976459503174
Loss: 3.233038902282715
Loss: 3.0184009075164795
Loss: 3.73130464553833
Loss: 3.3451461791992188
Loss: 3.0455780029296875
Loss: 3.3549444675445557
Loss: 3.146998405456543
Loss: 3.7069144248962402
Loss: 3.6756479740142822
Loss: 3.3494842052459717
Loss: 3.711559295654297
Loss: 3.452332019805908
Loss: 2.788506507873535
Loss: 3.197084903717041
Loss: 2.556156873703003
Loss: 3.173130750656128
Loss: 3.4149889945983887
Loss: 2.9915571212768555
Loss: 3.2890219688415527
Loss: 3.629992961883545
Loss: 4.026952266693115
Loss: 3.1470842361450195
Loss: 3.1335692405700684
Loss: 3.527066946029663
Loss: 3.145275115966797
Loss: 3.2104885578155518
Loss: 2.9181504249572754
Loss: 3.3270604610443115
Loss: 3.2293436527252197
Loss: 2.956820249557495
Loss: 3.16568660736084
Loss: 3.5203421115875244
Loss: 3.099456310272217
Loss: 3.302915096282959
Loss: 3.167644739151001
Loss: 3.619175672531128
Loss: 3.8318440914154053
Loss: 3.304668426513672
[Train] Epoch 36, accuracy 0.9940972222222222
[Eval] Epoch 36, accuracy 0.9875
Loss: 3.211522340774536
Loss: 3.2665891647338867
Loss: 3.521427631378174
Loss: 3.6118860244750977
Loss: 2.779104232788086
Loss: 3.7843406200408936
Loss: 3.1409871578216553
Loss: 3.418609619140625
Loss: 3.60239315032959
Loss: 3.562980890274048
Loss: 3.4939486980438232
Loss: 3.3048441410064697
Loss: 3.619938850402832
Loss: 3.675285577774048
Loss: 3.1341171264648438
Loss: 2.812952995300293
Loss: 3.205575942993164
Loss: 3.4201624393463135
Loss: 3.070312976837158
Loss: 2.5250167846679688
Loss: 3.5564470291137695
Loss: 3.3221144676208496
Loss: 3.432969808578491
Loss: 3.1498584747314453
Loss: 2.8108835220336914
Loss: 2.820172071456909
Loss: 2.9233102798461914
Loss: 3.0105857849121094
Loss: 2.917332410812378
Loss: 3.155184030532837
Loss: 3.161506414413452
Loss: 3.408085823059082
Loss: 2.781787157058716
Loss: 2.6184322834014893
Loss: 3.10721755027771
Loss: 4.0823845863342285
Loss: 2.8956408500671387
Loss: 2.991548538208008
Loss: 3.141329765319824
Loss: 3.0882680416107178
Loss: 3.004915237426758
Loss: 2.999512195587158
Loss: 3.3662426471710205
Loss: 2.933772325515747
Loss: 3.2246780395507812
[Train] Epoch 37, accuracy 0.9934027777777777
[Eval] Epoch 37, accuracy 0.9885416666666667
Model saved as model_weights_best.pth
Loss: 3.2899081707000732
Loss: 3.097825050354004
Loss: 3.706625461578369
Loss: 3.4325737953186035
Loss: 2.3740856647491455
Loss: 3.035274028778076
Loss: 3.176068067550659
Loss: 2.592250347137451
Loss: 2.9892046451568604
Loss: 3.0434892177581787
Loss: 3.0953638553619385
Loss: 2.982513189315796
Loss: 3.2501943111419678
Loss: 3.052278995513916
Loss: 3.282946825027466
Loss: 3.0540056228637695
Loss: 2.6907200813293457
Loss: 2.9108927249908447
Loss: 3.382444143295288
Loss: 2.732642412185669
Loss: 3.317464828491211
Loss: 3.342785358428955
Loss: 2.983431339263916
Loss: 3.3653104305267334
Loss: 2.9353277683258057
Loss: 3.566803216934204
Loss: 2.7765707969665527
Loss: 2.835747718811035
Loss: 3.1075761318206787
Loss: 3.1585195064544678
Loss: 3.167397975921631
Loss: 2.9475200176239014
Loss: 3.066089391708374
Loss: 3.064312219619751
Loss: 3.3688106536865234
Loss: 2.9637694358825684
Loss: 3.149824380874634
Loss: 2.7851903438568115
Loss: 2.655524969100952
Loss: 3.465785503387451
Loss: 2.807955741882324
Loss: 2.7786905765533447
Loss: 2.921379566192627
Loss: 3.1718945503234863
Loss: 2.8437392711639404
[Train] Epoch 38, accuracy 0.9941840277777778
[Eval] Epoch 38, accuracy 0.9899305555555555
Model saved as model_weights_best.pth
Loss: 2.8209574222564697
Loss: 2.8166935443878174
Loss: 2.710479974746704
Loss: 3.710714817047119
Loss: 2.659848213195801
Loss: 2.638965368270874
Loss: 2.6596004962921143
Loss: 3.064931869506836
Loss: 3.1395676136016846
Loss: 2.7752578258514404
Loss: 2.6292965412139893
Loss: 2.814354181289673
Loss: 2.7375943660736084
Loss: 2.6199281215667725
Loss: 2.8453969955444336
Loss: 3.084449529647827
Loss: 2.835625171661377
Loss: 3.149890422821045
Loss: 3.1444504261016846
Loss: 3.1248981952667236
Loss: 2.6242337226867676
Loss: 3.203383445739746
Loss: 3.246101140975952
Loss: 2.7994518280029297
Loss: 3.369816303253174
Loss: 2.7713916301727295
Loss: 3.653071403503418
Loss: 3.0387330055236816
Loss: 2.4216835498809814
Loss: 2.870791435241699
Loss: 2.6657121181488037
Loss: 3.2092790603637695
Loss: 3.0324134826660156
Loss: 2.7033283710479736
Loss: 2.8530819416046143
Loss: 3.1036996841430664
Loss: 2.7128283977508545
Loss: 3.120530128479004
Loss: 2.894216537475586
Loss: 3.253228187561035
Loss: 2.617440700531006
Loss: 3.050584077835083
Loss: 2.546496629714966
Loss: 2.502779006958008
Loss: 3.150402069091797
[Train] Epoch 39, accuracy 0.9942708333333333
[Eval] Epoch 39, accuracy 0.9878472222222222
Loss: 2.9848432540893555
Loss: 2.979429006576538
Loss: 2.8583226203918457
Loss: 2.418588399887085
Loss: 3.0861973762512207
Loss: 2.803832769393921
Loss: 3.1811535358428955
Loss: 2.6002023220062256
Loss: 2.996605396270752
Loss: 3.238614559173584
Loss: 2.6988770961761475
Loss: 2.8775346279144287
Loss: 2.794400930404663
Loss: 2.7342915534973145
Loss: 2.781845808029175
Loss: 2.793415069580078
Loss: 3.046395778656006
Loss: 2.6835594177246094
Loss: 2.5061886310577393
Loss: 2.5495595932006836
Loss: 3.150547742843628
Loss: 2.9141643047332764
Loss: 2.5706138610839844
Loss: 2.838193416595459
Loss: 2.5577311515808105
Loss: 2.3427600860595703
Loss: 3.1803972721099854
Loss: 2.2643349170684814
Loss: 3.0684702396392822
Loss: 3.3109986782073975
Loss: 2.7913200855255127
Loss: 2.7987825870513916
Loss: 3.4296064376831055
Loss: 2.4971437454223633
Loss: 2.7867846488952637
Loss: 2.672617197036743
Loss: 2.761200189590454
Loss: 3.0501303672790527
Loss: 3.0558876991271973
Loss: 2.9389476776123047
Loss: 2.764976978302002
Loss: 3.199922561645508
Loss: 3.539421558380127
Loss: 2.703491687774658
Loss: 2.7759904861450195
[Train] Epoch 40, accuracy 0.9947048611111111
[Eval] Epoch 40, accuracy 0.9881944444444445
Best accuracy: 0.990
