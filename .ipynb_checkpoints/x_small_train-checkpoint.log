Loss: 40.473506927490234
Loss: 40.7117805480957
Loss: 40.50568771362305
Loss: 40.62451934814453
Loss: 40.905548095703125
Loss: 40.97361373901367
Loss: 40.7419319152832
Loss: 40.68931198120117
Loss: 40.442291259765625
Loss: 40.71092987060547
Loss: 40.6378173828125
Loss: 40.516082763671875
Loss: 40.25224685668945
Loss: 40.79698944091797
Loss: 40.344573974609375
Loss: 40.53516387939453
Loss: 40.22313690185547
Loss: 39.95957946777344
Loss: 40.1982307434082
Loss: 40.13636016845703
Loss: 39.869361877441406
Loss: 40.00575637817383
Loss: 39.920352935791016
Loss: 39.858673095703125
Loss: 40.01247024536133
Loss: 39.810997009277344
Loss: 39.397125244140625
Loss: 39.15945053100586
Loss: 39.01081466674805
Loss: 39.450965881347656
Loss: 39.79001235961914
Loss: 39.36296844482422
Loss: 39.10786819458008
Loss: 38.95718002319336
Loss: 39.02622985839844
Loss: 39.063697814941406
Loss: 39.18874740600586
Loss: 39.234439849853516
Loss: 39.17314910888672
Loss: 39.326744079589844
Loss: 38.87766647338867
Loss: 38.969600677490234
Loss: 38.858402252197266
Loss: 38.62251663208008
Loss: 38.6955680847168
Loss: 38.80175018310547
Loss: 38.72596740722656
Loss: 38.35343551635742
Loss: 38.727901458740234
Loss: 38.265995025634766
Loss: 38.554466247558594
Loss: 38.52708053588867
Loss: 38.29602813720703
Loss: 38.364437103271484
Loss: 38.29765701293945
Loss: 38.350887298583984
Loss: 38.23922348022461
Loss: 37.945533752441406
Loss: 38.180904388427734
Loss: 38.243194580078125
Loss: 38.18037796020508
Loss: 38.15100860595703
Loss: 37.98659133911133
Loss: 38.05382537841797
Loss: 38.08724594116211
Loss: 38.100196838378906
Loss: 37.865753173828125
Loss: 37.967689514160156
Loss: 37.69349670410156
Loss: 37.95403289794922
Loss: 37.973819732666016
Loss: 37.89722442626953
Loss: 37.80794143676758
Loss: 37.675743103027344
Loss: 37.78812789916992
Loss: 38.018253326416016
Loss: 37.66419982910156
Loss: 37.67509078979492
Loss: 37.58186340332031
Loss: 37.469566345214844
Loss: 37.72142791748047
Loss: 37.605567932128906
Loss: 37.553672790527344
Loss: 37.46033477783203
Loss: 37.5765495300293
Loss: 37.86775588989258
Loss: 37.556610107421875
Loss: 37.771114349365234
Loss: 37.46432113647461
Loss: 37.398311614990234
[Train] Epoch 1, accuracy 0.0020833333333333333
[Eval] Epoch 1, accuracy 0.004166666666666667
Model saved as x_small_model_weights_best.pth
Loss: 37.793949127197266
Loss: 37.18966293334961
Loss: 37.48960876464844
Loss: 37.42298126220703
Loss: 37.37725067138672
Loss: 37.76803970336914
Loss: 37.33427429199219
Loss: 37.322235107421875
Loss: 37.403316497802734
Loss: 37.257835388183594
Loss: 37.24985885620117
Loss: 37.23548889160156
Loss: 37.296791076660156
Loss: 37.187904357910156
Loss: 37.39052963256836
Loss: 37.24248123168945
Loss: 37.35721206665039
Loss: 37.29387664794922
Loss: 37.480350494384766
Loss: 37.234947204589844
Loss: 37.32231521606445
Loss: 37.25661849975586
Loss: 37.02800750732422
Loss: 37.357810974121094
Loss: 37.27115249633789
Loss: 37.26237869262695
Loss: 37.103275299072266
Loss: 37.277305603027344
Loss: 37.07699203491211
Loss: 37.27293395996094
Loss: 37.348873138427734
Loss: 37.13136672973633
Loss: 37.23467254638672
Loss: 37.289466857910156
Loss: 37.3845329284668
Loss: 37.07837677001953
Loss: 37.1798210144043
Loss: 36.909393310546875
Loss: 37.239166259765625
Loss: 37.26070022583008
Loss: 37.06565856933594
Loss: 37.00809097290039
Loss: 37.36739730834961
Loss: 37.09433364868164
Loss: 37.28664779663086
Loss: 37.14289474487305
Loss: 37.38669967651367
Loss: 37.264225006103516
Loss: 36.983585357666016
Loss: 37.20232009887695
Loss: 37.26731872558594
Loss: 37.035282135009766
Loss: 37.202857971191406
Loss: 37.17082595825195
Loss: 37.10443115234375
Loss: 37.10622024536133
Loss: 37.163352966308594
Loss: 37.142799377441406
Loss: 37.22391891479492
Loss: 37.137184143066406
Loss: 37.046146392822266
Loss: 37.1717643737793
Loss: 37.01048278808594
Loss: 37.08523178100586
Loss: 37.14571762084961
Loss: 36.908538818359375
Loss: 37.11127471923828
Loss: 37.13615036010742
Loss: 37.137725830078125
Loss: 37.17654800415039
Loss: 36.954978942871094
Loss: 36.94462203979492
Loss: 36.93906784057617
Loss: 36.96739959716797
Loss: 37.0655632019043
Loss: 37.09101104736328
Loss: 37.18108367919922
Loss: 37.27789306640625
Loss: 37.08872985839844
Loss: 36.836273193359375
Loss: 36.98356628417969
Loss: 36.89830017089844
Loss: 37.069820404052734
Loss: 36.87685012817383
Loss: 36.98058319091797
Loss: 37.044673919677734
Loss: 37.04336929321289
Loss: 37.0245475769043
Loss: 37.051483154296875
Loss: 36.964107513427734
[Train] Epoch 2, accuracy 0.004861111111111111
[Eval] Epoch 2, accuracy 0.005208333333333333
Model saved as x_small_model_weights_best.pth
Loss: 37.02328872680664
Loss: 36.949893951416016
Loss: 36.82801055908203
Loss: 36.894004821777344
Loss: 36.93012619018555
Loss: 36.9562873840332
Loss: 36.97699737548828
Loss: 37.073089599609375
Loss: 36.97262954711914
Loss: 36.995845794677734
Loss: 36.89712905883789
Loss: 36.98759460449219
Loss: 36.92668151855469
Loss: 37.16426467895508
Loss: 36.8585319519043
Loss: 36.92322540283203
Loss: 36.981239318847656
Loss: 36.95811080932617
Loss: 36.79869842529297
Loss: 37.0771369934082
Loss: 37.00995635986328
Loss: 36.958984375
Loss: 36.949913024902344
Loss: 36.90554428100586
Loss: 36.98066711425781
Loss: 36.88924026489258
Loss: 37.029212951660156
Loss: 36.97872543334961
Loss: 36.8350715637207
Loss: 36.858642578125
Loss: 36.74324035644531
Loss: 36.84840774536133
Loss: 36.82142639160156
Loss: 36.90089797973633
Loss: 37.062068939208984
Loss: 37.02671813964844
Loss: 37.00472640991211
Loss: 37.00419616699219
Loss: 36.781314849853516
Loss: 36.96359634399414
Loss: 36.85606002807617
Loss: 36.792293548583984
Loss: 36.88965606689453
Loss: 36.79939270019531
Loss: 37.04747009277344
Loss: 36.892452239990234
Loss: 36.89157485961914
Loss: 36.869956970214844
Loss: 36.90729522705078
Loss: 36.93391418457031
Loss: 37.047096252441406
Loss: 36.91802978515625
Loss: 36.762550354003906
Loss: 36.90370559692383
Loss: 36.875003814697266
Loss: 36.82435607910156
Loss: 36.70587158203125
Loss: 36.972232818603516
Loss: 36.896278381347656
Loss: 36.81732940673828
Loss: 36.81572341918945
Loss: 36.661563873291016
Loss: 36.589481353759766
Loss: 36.80746078491211
Loss: 36.82279968261719
Loss: 36.671730041503906
Loss: 36.79898452758789
Loss: 36.77827072143555
Loss: 36.98446273803711
Loss: 36.98057174682617
Loss: 37.02294921875
Loss: 36.694602966308594
Loss: 36.93687438964844
Loss: 36.71569061279297
Loss: 36.78303909301758
Loss: 36.970829010009766
Loss: 36.81675338745117
Loss: 36.725711822509766
Loss: 36.778907775878906
Loss: 36.74958801269531
Loss: 36.771080017089844
Loss: 36.783870697021484
Loss: 36.67679977416992
Loss: 36.64287185668945
Loss: 36.786773681640625
Loss: 36.66508483886719
Loss: 36.71027755737305
Loss: 36.67574691772461
Loss: 36.67485809326172
Loss: 36.641815185546875
[Train] Epoch 3, accuracy 0.008940972222222222
[Eval] Epoch 3, accuracy 0.013541666666666667
Model saved as x_small_model_weights_best.pth
Loss: 36.53335952758789
Loss: 36.486175537109375
Loss: 36.70035171508789
Loss: 36.57334518432617
Loss: 36.670875549316406
Loss: 36.606082916259766
Loss: 36.730308532714844
Loss: 36.89263153076172
Loss: 36.575443267822266
Loss: 36.754295349121094
Loss: 36.58763885498047
Loss: 36.50071334838867
Loss: 36.6408805847168
Loss: 36.5020751953125
Loss: 36.75459289550781
Loss: 36.66519546508789
Loss: 36.63618850708008
Loss: 36.60678482055664
Loss: 36.784332275390625
Loss: 36.52293395996094
Loss: 36.75627899169922
Loss: 36.64399719238281
Loss: 36.65270233154297
Loss: 36.51905822753906
Loss: 36.55859375
Loss: 36.68052291870117
Loss: 36.50888442993164
Loss: 36.50680160522461
Loss: 36.785640716552734
Loss: 36.68170928955078
Loss: 36.56945037841797
Loss: 36.57714080810547
Loss: 36.5525016784668
Loss: 36.47136306762695
Loss: 36.30720138549805
Loss: 36.410736083984375
Loss: 36.502193450927734
Loss: 36.555076599121094
Loss: 36.67167282104492
Loss: 36.49674606323242
Loss: 36.51508712768555
Loss: 36.65686798095703
Loss: 36.41204071044922
Loss: 36.66646194458008
Loss: 36.573341369628906
Loss: 36.492431640625
Loss: 36.55500411987305
Loss: 36.48543930053711
Loss: 36.47658920288086
Loss: 36.56745147705078
Loss: 36.72037124633789
Loss: 36.64269256591797
Loss: 36.553619384765625
Loss: 36.52789306640625
Loss: 36.60100555419922
Loss: 36.56604766845703
Loss: 36.52613830566406
Loss: 36.684574127197266
Loss: 36.63743209838867
Loss: 36.58170700073242
Loss: 36.61642837524414
Loss: 36.559226989746094
Loss: 36.478790283203125
Loss: 36.47303009033203
Loss: 36.38722229003906
Loss: 36.50910568237305
Loss: 36.548545837402344
Loss: 36.50342559814453
Loss: 36.413299560546875
Loss: 36.51116943359375
Loss: 36.290138244628906
Loss: 36.4474983215332
Loss: 36.4380989074707
Loss: 36.441017150878906
Loss: 36.54616928100586
Loss: 36.6396369934082
Loss: 36.517024993896484
Loss: 36.43509292602539
Loss: 36.44670486450195
Loss: 36.422462463378906
Loss: 36.42218017578125
Loss: 36.37931823730469
Loss: 36.43442916870117
Loss: 36.404945373535156
Loss: 36.33126449584961
Loss: 36.5322380065918
Loss: 36.456241607666016
Loss: 36.40949249267578
Loss: 36.40397644042969
Loss: 36.3092155456543
[Train] Epoch 4, accuracy 0.01935763888888889
[Eval] Epoch 4, accuracy 0.029861111111111113
Model saved as x_small_model_weights_best.pth
Loss: 36.32442092895508
Loss: 36.3087272644043
Loss: 36.48826217651367
Loss: 36.31004333496094
Loss: 36.43388748168945
Loss: 36.392513275146484
Loss: 36.28500747680664
Loss: 36.44890213012695
Loss: 36.230567932128906
Loss: 36.386844635009766
Loss: 36.37031936645508
Loss: 36.244808197021484
Loss: 36.2635498046875
Loss: 36.23666763305664
Loss: 36.349544525146484
Loss: 36.27116394042969
Loss: 36.28791046142578
Loss: 36.174049377441406
Loss: 36.349334716796875
Loss: 36.3239860534668
Loss: 36.3825569152832
Loss: 36.067604064941406
Loss: 36.36677932739258
Loss: 36.25623321533203
Loss: 36.252166748046875
Loss: 36.12892532348633
Loss: 36.388851165771484
Loss: 36.327484130859375
Loss: 36.440460205078125
Loss: 36.237674713134766
Loss: 36.248172760009766
Loss: 36.31085968017578
Loss: 36.3841667175293
Loss: 36.218143463134766
Loss: 36.58841323852539
Loss: 36.07598114013672
Loss: 36.49271774291992
Loss: 36.30984878540039
Loss: 36.36361312866211
Loss: 36.26329803466797
Loss: 36.281982421875
Loss: 36.16516876220703
Loss: 36.13467788696289
Loss: 35.992313385009766
Loss: 36.190311431884766
Loss: 36.33456039428711
Loss: 36.200782775878906
Loss: 36.349769592285156
Loss: 36.34275436401367
Loss: 36.255271911621094
Loss: 36.17438507080078
Loss: 36.29603576660156
Loss: 36.10750198364258
Loss: 36.17311477661133
Loss: 36.28485107421875
Loss: 36.45387649536133
Loss: 36.23963165283203
Loss: 36.18814468383789
Loss: 36.243282318115234
Loss: 36.28375244140625
Loss: 36.1949577331543
Loss: 36.265106201171875
Loss: 36.30447006225586
Loss: 36.05223083496094
Loss: 36.15296173095703
Loss: 36.109375
Loss: 36.1108512878418
Loss: 36.24264144897461
Loss: 36.05929946899414
Loss: 36.01356887817383
Loss: 36.305023193359375
Loss: 35.91144561767578
Loss: 36.19118881225586
Loss: 36.09977340698242
Loss: 36.188411712646484
Loss: 36.21915054321289
Loss: 36.14612579345703
Loss: 36.03675842285156
Loss: 36.0142936706543
Loss: 36.040645599365234
Loss: 36.07809829711914
Loss: 35.99387741088867
Loss: 36.022865295410156
Loss: 36.19614791870117
Loss: 36.282325744628906
Loss: 36.25629425048828
Loss: 36.0214958190918
Loss: 36.07801818847656
Loss: 36.2379264831543
Loss: 36.0050163269043
[Train] Epoch 5, accuracy 0.03550347222222222
[Eval] Epoch 5, accuracy 0.05416666666666667
Model saved as x_small_model_weights_best.pth
Loss: 35.83675765991211
Loss: 35.77580642700195
Loss: 35.977142333984375
Loss: 35.69276428222656
Loss: 35.808265686035156
Loss: 36.076210021972656
Loss: 35.9869270324707
Loss: 35.92586135864258
Loss: 36.061920166015625
Loss: 35.8754768371582
Loss: 35.96483612060547
Loss: 35.807071685791016
Loss: 35.962913513183594
Loss: 36.075321197509766
Loss: 35.90530014038086
Loss: 35.78047180175781
Loss: 36.074058532714844
Loss: 36.044586181640625
Loss: 36.06999206542969
Loss: 36.00316619873047
Loss: 35.949119567871094
Loss: 35.93809509277344
Loss: 35.89560317993164
Loss: 35.84992599487305
Loss: 35.91468048095703
Loss: 35.81268310546875
Loss: 35.8858642578125
Loss: 35.910003662109375
Loss: 35.76630401611328
Loss: 35.94601058959961
Loss: 35.98460388183594
Loss: 35.7381706237793
Loss: 35.91539764404297
Loss: 35.79003143310547
Loss: 35.83439636230469
Loss: 35.855289459228516
Loss: 35.726165771484375
Loss: 35.875755310058594
Loss: 35.89701843261719
Loss: 35.71179962158203
Loss: 35.92626190185547
Loss: 35.85615921020508
Loss: 35.84403991699219
Loss: 35.822784423828125
Loss: 35.76630783081055
Loss: 35.80546188354492
Loss: 35.795379638671875
Loss: 36.03749465942383
Loss: 35.60652160644531
Loss: 35.625247955322266
Loss: 35.809791564941406
Loss: 35.75651931762695
Loss: 35.757747650146484
Loss: 35.65800094604492
Loss: 35.661075592041016
Loss: 35.6004524230957
Loss: 35.73341369628906
Loss: 35.51837158203125
Loss: 35.714576721191406
Loss: 35.994449615478516
Loss: 35.43629455566406
Loss: 35.83203887939453
Loss: 35.86896514892578
Loss: 35.87248229980469
Loss: 35.81952667236328
Loss: 35.57466125488281
Loss: 35.635406494140625
Loss: 35.512481689453125
Loss: 35.65666198730469
Loss: 35.59971237182617
Loss: 35.406394958496094
Loss: 35.50337219238281
Loss: 35.58084487915039
Loss: 35.570064544677734
Loss: 35.546695709228516
Loss: 35.55059051513672
Loss: 35.669189453125
Loss: 35.3800048828125
Loss: 35.362571716308594
Loss: 35.74662399291992
Loss: 35.575416564941406
Loss: 35.7194709777832
Loss: 35.543338775634766
Loss: 35.42477798461914
Loss: 35.328521728515625
Loss: 35.709815979003906
Loss: 35.62059783935547
Loss: 35.60838317871094
Loss: 35.751014709472656
Loss: 35.803497314453125
[Train] Epoch 6, accuracy 0.07178819444444444
[Eval] Epoch 6, accuracy 0.11770833333333333
Model saved as x_small_model_weights_best.pth
Loss: 35.33467483520508
Loss: 35.58170700073242
Loss: 35.5220832824707
Loss: 35.27370071411133
Loss: 35.5164794921875
Loss: 35.5284309387207
Loss: 35.27811813354492
Loss: 35.557682037353516
Loss: 35.39259719848633
Loss: 35.285587310791016
Loss: 35.24094772338867
Loss: 35.353614807128906
Loss: 35.80622100830078
Loss: 35.292137145996094
Loss: 35.1793098449707
Loss: 35.543052673339844
Loss: 35.45027542114258
Loss: 35.134910583496094
Loss: 35.39946365356445
Loss: 35.11750411987305
Loss: 35.331298828125
Loss: 35.425079345703125
Loss: 35.106563568115234
Loss: 35.30813217163086
Loss: 35.28486633300781
Loss: 35.35613250732422
Loss: 35.133094787597656
Loss: 35.52360534667969
Loss: 35.411720275878906
Loss: 35.5482063293457
Loss: 35.19643020629883
Loss: 35.568023681640625
Loss: 35.225067138671875
Loss: 35.45475769042969
Loss: 35.398651123046875
Loss: 35.281707763671875
Loss: 35.17945861816406
Loss: 35.261539459228516
Loss: 35.449684143066406
Loss: 35.30128860473633
Loss: 35.38951873779297
Loss: 35.15563201904297
Loss: 35.31602478027344
Loss: 35.15922546386719
Loss: 35.29167175292969
Loss: 35.354034423828125
Loss: 35.033878326416016
Loss: 35.18989562988281
Loss: 35.34929275512695
Loss: 35.31201171875
Loss: 35.10036849975586
Loss: 35.10329055786133
Loss: 35.340911865234375
Loss: 35.369686126708984
Loss: 35.02094650268555
Loss: 34.920921325683594
Loss: 34.8721809387207
Loss: 35.435245513916016
Loss: 35.19729232788086
Loss: 35.2560920715332
Loss: 35.084835052490234
Loss: 35.006595611572266
Loss: 35.453025817871094
Loss: 35.16612243652344
Loss: 34.880558013916016
Loss: 34.880516052246094
Loss: 35.065208435058594
Loss: 34.958744049072266
Loss: 34.7864875793457
Loss: 34.971858978271484
Loss: 34.99201965332031
Loss: 35.00358200073242
Loss: 34.864925384521484
Loss: 34.89485168457031
Loss: 34.77973175048828
Loss: 34.90773391723633
Loss: 35.083099365234375
Loss: 35.109291076660156
Loss: 34.79788589477539
Loss: 34.87561798095703
Loss: 34.6577262878418
Loss: 34.79031753540039
Loss: 34.98859786987305
Loss: 34.990760803222656
Loss: 34.90250778198242
Loss: 35.17779541015625
Loss: 34.85859298706055
Loss: 35.00852584838867
Loss: 35.032005310058594
Loss: 34.88559341430664
[Train] Epoch 7, accuracy 0.1449652777777778
[Eval] Epoch 7, accuracy 0.21284722222222222
Model saved as x_small_model_weights_best.pth
Loss: 34.85179901123047
Loss: 34.61524963378906
Loss: 34.78110122680664
Loss: 34.832759857177734
Loss: 34.60002136230469
Loss: 34.54917526245117
Loss: 34.9356575012207
Loss: 34.55091857910156
Loss: 34.48947525024414
Loss: 34.54882049560547
Loss: 34.651275634765625
Loss: 34.83951950073242
Loss: 34.86335372924805
Loss: 34.69370651245117
Loss: 34.65016555786133
Loss: 34.87154006958008
Loss: 34.87146759033203
Loss: 34.62751388549805
Loss: 34.68388748168945
Loss: 34.47386932373047
Loss: 34.7485466003418
Loss: 34.60264587402344
Loss: 34.73563003540039
Loss: 34.3509407043457
Loss: 34.57637405395508
Loss: 34.443626403808594
Loss: 34.72400665283203
Loss: 34.32305908203125
Loss: 34.60346221923828
Loss: 34.53302764892578
Loss: 34.50362777709961
Loss: 34.380130767822266
Loss: 34.6087532043457
Loss: 34.686424255371094
Loss: 34.844112396240234
Loss: 34.5396614074707
Loss: 34.58572006225586
Loss: 34.18816375732422
Loss: 34.29219055175781
Loss: 34.69186019897461
Loss: 34.445369720458984
Loss: 34.73958206176758
Loss: 34.33460998535156
Loss: 34.37141418457031
Loss: 34.52976989746094
Loss: 34.33387756347656
Loss: 34.78498458862305
Loss: 34.489864349365234
Loss: 34.5406494140625
Loss: 34.404747009277344
Loss: 34.33773422241211
Loss: 34.38858413696289
Loss: 34.398033142089844
Loss: 34.57370376586914
Loss: 34.346923828125
Loss: 34.485984802246094
Loss: 34.135406494140625
Loss: 34.310943603515625
Loss: 34.36659240722656
Loss: 34.117862701416016
Loss: 33.97200012207031
Loss: 34.262752532958984
Loss: 34.23707962036133
Loss: 33.84091567993164
Loss: 34.09159851074219
Loss: 34.25169372558594
Loss: 33.86440658569336
Loss: 33.99671936035156
Loss: 34.01728820800781
Loss: 34.403968811035156
Loss: 34.03522872924805
Loss: 34.16878128051758
Loss: 34.090885162353516
Loss: 33.93326187133789
Loss: 34.044071197509766
Loss: 34.08226013183594
Loss: 34.16937255859375
Loss: 33.8207893371582
Loss: 34.24871063232422
Loss: 34.36615753173828
Loss: 34.042091369628906
Loss: 34.22337341308594
Loss: 33.742000579833984
Loss: 33.61824035644531
Loss: 34.007102966308594
Loss: 34.10518264770508
Loss: 33.99251937866211
Loss: 33.91176223754883
Loss: 34.15315628051758
Loss: 33.852943420410156
[Train] Epoch 8, accuracy 0.26614583333333336
[Eval] Epoch 8, accuracy 0.35
Model saved as x_small_model_weights_best.pth
Loss: 33.79035568237305
Loss: 33.980960845947266
Loss: 33.69285202026367
Loss: 34.23017883300781
Loss: 33.92937088012695
Loss: 33.72484588623047
Loss: 33.526771545410156
Loss: 33.80806350708008
Loss: 33.868221282958984
Loss: 33.779842376708984
Loss: 33.85535430908203
Loss: 33.65476989746094
Loss: 33.76003646850586
Loss: 33.99324417114258
Loss: 33.64198684692383
Loss: 33.62175369262695
Loss: 33.539798736572266
Loss: 33.83050537109375
Loss: 33.65331268310547
Loss: 33.61885070800781
Loss: 33.83474349975586
Loss: 33.48031997680664
Loss: 33.636165618896484
Loss: 33.23998260498047
Loss: 33.3753662109375
Loss: 33.75580978393555
Loss: 33.64117431640625
Loss: 33.60297393798828
Loss: 33.85580825805664
Loss: 33.77165985107422
Loss: 33.744232177734375
Loss: 33.38067626953125
Loss: 33.306976318359375
Loss: 33.280174255371094
Loss: 33.303466796875
Loss: 33.56986999511719
Loss: 33.8454704284668
Loss: 33.641544342041016
Loss: 33.56118392944336
Loss: 33.06928634643555
Loss: 33.38390350341797
Loss: 33.288394927978516
Loss: 33.63068389892578
Loss: 33.08738708496094
Loss: 33.47541809082031
Loss: 33.459678649902344
Loss: 32.97566223144531
Loss: 33.209815979003906
Loss: 33.185794830322266
Loss: 33.366580963134766
Loss: 33.356170654296875
Loss: 33.17661666870117
Loss: 33.11418533325195
Loss: 33.23120880126953
Loss: 33.16802978515625
Loss: 33.37298583984375
Loss: 33.38208770751953
Loss: 33.498294830322266
Loss: 32.86322784423828
Loss: 33.08062744140625
Loss: 33.74824142456055
Loss: 33.23038864135742
Loss: 33.18730926513672
Loss: 33.308021545410156
Loss: 33.30223846435547
Loss: 33.17428207397461
Loss: 32.795989990234375
Loss: 33.23088073730469
Loss: 33.11257553100586
Loss: 33.187705993652344
Loss: 33.06352615356445
Loss: 33.31383514404297
Loss: 32.991905212402344
Loss: 33.03190612792969
Loss: 32.87290954589844
Loss: 33.014583587646484
Loss: 32.84354019165039
Loss: 32.73881912231445
Loss: 32.68132019042969
Loss: 32.835750579833984
Loss: 32.97288513183594
Loss: 33.08253860473633
Loss: 32.98555374145508
Loss: 33.093074798583984
Loss: 33.0115852355957
Loss: 32.61748504638672
Loss: 32.92806625366211
Loss: 32.63747787475586
Loss: 32.871150970458984
Loss: 32.6157341003418
[Train] Epoch 9, accuracy 0.41727430555555556
[Eval] Epoch 9, accuracy 0.49201388888888886
Model saved as x_small_model_weights_best.pth
Loss: 32.735408782958984
Loss: 32.39694595336914
Loss: 32.4303092956543
Loss: 32.387359619140625
Loss: 32.505184173583984
Loss: 32.67180252075195
Loss: 32.672325134277344
Loss: 32.703269958496094
Loss: 32.59587860107422
Loss: 32.35784912109375
Loss: 32.4742546081543
Loss: 32.695838928222656
Loss: 32.47995376586914
Loss: 32.70610046386719
Loss: 32.635223388671875
Loss: 32.57443618774414
Loss: 32.57221603393555
Loss: 32.46981430053711
Loss: 32.52550506591797
Loss: 32.186004638671875
Loss: 32.12120056152344
Loss: 32.44913864135742
Loss: 32.11637496948242
Loss: 32.1284065246582
Loss: 32.402645111083984
Loss: 32.54121017456055
Loss: 32.521759033203125
Loss: 32.36545181274414
Loss: 32.72333526611328
Loss: 32.24787139892578
Loss: 32.21725082397461
Loss: 32.23127365112305
Loss: 32.026947021484375
Loss: 32.30034255981445
Loss: 32.160701751708984
Loss: 32.63791275024414
Loss: 32.13087463378906
Loss: 32.45020294189453
Loss: 32.54827880859375
Loss: 32.22610092163086
Loss: 32.37193298339844
Loss: 32.260231018066406
Loss: 32.04665756225586
Loss: 31.678667068481445
Loss: 32.27703094482422
Loss: 32.02091979980469
Loss: 32.07752990722656
Loss: 32.662845611572266
Loss: 31.888378143310547
Loss: 31.93585205078125
Loss: 32.052162170410156
Loss: 31.79197883605957
Loss: 32.03437042236328
Loss: 32.06284713745117
Loss: 32.25525665283203
Loss: 31.9575138092041
Loss: 31.977872848510742
Loss: 32.11906051635742
Loss: 31.627042770385742
Loss: 31.9555721282959
Loss: 32.05718994140625
Loss: 31.980602264404297
Loss: 31.653545379638672
Loss: 32.042720794677734
Loss: 32.117698669433594
Loss: 32.04012680053711
Loss: 31.475223541259766
Loss: 31.545352935791016
Loss: 31.909900665283203
Loss: 32.016212463378906
Loss: 32.032596588134766
Loss: 31.842451095581055
Loss: 31.54058265686035
Loss: 32.01966094970703
Loss: 32.11510467529297
Loss: 31.916269302368164
Loss: 31.647140502929688
Loss: 31.856990814208984
Loss: 31.989330291748047
Loss: 31.33600616455078
Loss: 31.67513656616211
Loss: 32.000362396240234
Loss: 31.346858978271484
Loss: 31.86404037475586
Loss: 31.614416122436523
Loss: 31.455997467041016
Loss: 31.532270431518555
Loss: 32.13119888305664
Loss: 31.999347686767578
Loss: 31.65175437927246
[Train] Epoch 10, accuracy 0.5825520833333333
[Eval] Epoch 10, accuracy 0.6385416666666667
Model saved as x_small_model_weights_best.pth
Loss: 31.505876541137695
Loss: 31.223514556884766
Loss: 31.59756088256836
Loss: 31.35250473022461
Loss: 31.611377716064453
Loss: 31.10416603088379
Loss: 31.371994018554688
Loss: 31.42757797241211
Loss: 31.15306854248047
Loss: 31.34926986694336
Loss: 31.509929656982422
Loss: 31.208969116210938
Loss: 31.35512351989746
Loss: 31.480854034423828
Loss: 31.29159927368164
Loss: 31.078723907470703
Loss: 30.655458450317383
Loss: 30.869123458862305
Loss: 31.1251277923584
Loss: 30.742164611816406
Loss: 31.232559204101562
Loss: 30.989397048950195
Loss: 31.36638069152832
Loss: 30.847978591918945
Loss: 30.85513687133789
Loss: 31.220075607299805
Loss: 31.186752319335938
Loss: 30.922874450683594
Loss: 30.93617057800293
Loss: 31.181659698486328
Loss: 31.674907684326172
Loss: 31.041959762573242
Loss: 30.892236709594727
Loss: 31.016555786132812
Loss: 31.378746032714844
Loss: 31.04865264892578
Loss: 31.22136878967285
Loss: 30.63576316833496
Loss: 31.11144256591797
Loss: 30.912160873413086
Loss: 30.60577964782715
Loss: 31.371246337890625
Loss: 30.564523696899414
Loss: 31.02040672302246
Loss: 30.670547485351562
Loss: 30.681697845458984
Loss: 30.988683700561523
Loss: 30.532922744750977
Loss: 30.426345825195312
Loss: 30.619508743286133
Loss: 30.853763580322266
Loss: 30.8686580657959
Loss: 30.82404136657715
Loss: 30.72100830078125
Loss: 30.565616607666016
Loss: 31.067794799804688
Loss: 31.182811737060547
Loss: 30.06940269470215
Loss: 31.00872230529785
Loss: 30.59988021850586
Loss: 30.666610717773438
Loss: 30.650455474853516
Loss: 30.21596336364746
Loss: 30.41942024230957
Loss: 30.678287506103516
Loss: 30.400272369384766
Loss: 31.07489013671875
Loss: 30.877790451049805
Loss: 30.50910186767578
Loss: 30.63616371154785
Loss: 30.396364212036133
Loss: 30.251310348510742
Loss: 30.370616912841797
Loss: 30.752756118774414
Loss: 30.91761016845703
Loss: 30.275320053100586
Loss: 30.384937286376953
Loss: 30.424274444580078
Loss: 30.125709533691406
Loss: 30.246646881103516
Loss: 30.3891544342041
Loss: 30.20817756652832
Loss: 30.740365982055664
Loss: 30.17915916442871
Loss: 30.492456436157227
Loss: 30.132822036743164
Loss: 30.190044403076172
Loss: 29.951427459716797
Loss: 30.12943458557129
Loss: 29.812623977661133
[Train] Epoch 11, accuracy 0.7141493055555556
[Eval] Epoch 11, accuracy 0.6645833333333333
Model saved as x_small_model_weights_best.pth
Loss: 29.120521545410156
Loss: 29.566133499145508
Loss: 29.863975524902344
Loss: 29.722583770751953
Loss: 29.930465698242188
Loss: 29.754989624023438
Loss: 30.536468505859375
Loss: 30.050270080566406
Loss: 29.759729385375977
Loss: 29.708209991455078
Loss: 29.519657135009766
Loss: 29.37488555908203
Loss: 29.84342384338379
Loss: 29.414371490478516
Loss: 29.611421585083008
Loss: 30.139944076538086
Loss: 29.65797996520996
Loss: 29.557689666748047
Loss: 29.693195343017578
Loss: 29.748607635498047
Loss: 29.46190071105957
Loss: 29.71135902404785
Loss: 30.26161766052246
Loss: 29.631107330322266
Loss: 29.18800926208496
Loss: 29.642738342285156
Loss: 29.76796531677246
Loss: 29.12602424621582
Loss: 30.046506881713867
Loss: 29.21356964111328
Loss: 29.862834930419922
Loss: 29.412996292114258
Loss: 29.914098739624023
Loss: 29.663318634033203
Loss: 29.567453384399414
Loss: 29.742040634155273
Loss: 29.161090850830078
Loss: 29.697818756103516
Loss: 29.244417190551758
Loss: 29.718460083007812
Loss: 29.742277145385742
Loss: 29.326074600219727
Loss: 29.59049415588379
Loss: 29.581127166748047
Loss: 29.39641571044922
Loss: 29.206470489501953
Loss: 29.737516403198242
Loss: 29.10087013244629
Loss: 29.28553581237793
Loss: 29.785324096679688
Loss: 29.233461380004883
Loss: 29.397001266479492
Loss: 29.36602210998535
Loss: 29.683698654174805
Loss: 29.54741859436035
Loss: 28.643157958984375
Loss: 28.93771743774414
Loss: 29.155759811401367
Loss: 29.08588981628418
Loss: 28.90549087524414
Loss: 29.393146514892578
Loss: 28.81679916381836
Loss: 29.433170318603516
Loss: 29.299991607666016
Loss: 29.062753677368164
Loss: 29.044702529907227
Loss: 29.221040725708008
Loss: 28.883771896362305
Loss: 28.690753936767578
Loss: 29.000532150268555
Loss: 28.935483932495117
Loss: 29.400306701660156
Loss: 29.10063362121582
Loss: 28.558876037597656
Loss: 28.420564651489258
Loss: 29.491430282592773
Loss: 28.04254722595215
Loss: 28.911745071411133
Loss: 29.275089263916016
Loss: 28.46228790283203
Loss: 28.7502498626709
Loss: 29.076595306396484
Loss: 28.332284927368164
Loss: 28.235963821411133
Loss: 28.292903900146484
Loss: 28.91314697265625
Loss: 28.778274536132812
Loss: 28.368513107299805
Loss: 28.309158325195312
Loss: 28.67369842529297
[Train] Epoch 12, accuracy 0.7961805555555556
[Eval] Epoch 12, accuracy 0.7875
Model saved as x_small_model_weights_best.pth
Loss: 28.505474090576172
Loss: 28.26034164428711
Loss: 27.854373931884766
Loss: 28.975875854492188
Loss: 28.14480972290039
Loss: 28.229928970336914
Loss: 28.31777000427246
Loss: 28.5048828125
Loss: 28.840965270996094
Loss: 27.466962814331055
Loss: 28.31679916381836
Loss: 28.4355525970459
Loss: 28.225067138671875
Loss: 27.939682006835938
Loss: 28.411497116088867
Loss: 28.868741989135742
Loss: 27.812484741210938
Loss: 28.604656219482422
Loss: 28.226652145385742
Loss: 27.614402770996094
Loss: 28.309402465820312
Loss: 28.91404151916504
Loss: 27.883543014526367
Loss: 28.411739349365234
Loss: 27.490007400512695
Loss: 27.50728988647461
Loss: 28.246994018554688
Loss: 27.285871505737305
Loss: 27.61281967163086
Loss: 27.969091415405273
Loss: 27.475696563720703
Loss: 27.895339965820312
Loss: 27.708053588867188
Loss: 28.173641204833984
Loss: 27.693843841552734
Loss: 27.743467330932617
Loss: 27.535924911499023
Loss: 27.52529525756836
Loss: 26.771379470825195
Loss: 27.558534622192383
Loss: 27.95024299621582
Loss: 27.640483856201172
Loss: 27.826194763183594
Loss: 27.403316497802734
Loss: 27.155412673950195
Loss: 27.78781509399414
Loss: 27.192838668823242
Loss: 27.38468360900879
Loss: 27.182796478271484
Loss: 27.00914764404297
Loss: 27.43494415283203
Loss: 27.971492767333984
Loss: 27.26089859008789
Loss: 27.691360473632812
Loss: 27.43708610534668
Loss: 28.253110885620117
Loss: 27.339599609375
Loss: 27.7918643951416
Loss: 26.69715118408203
Loss: 26.869775772094727
Loss: 26.890344619750977
Loss: 27.002559661865234
Loss: 27.247949600219727
Loss: 27.3257999420166
Loss: 27.248228073120117
Loss: 26.39799690246582
Loss: 27.18094825744629
Loss: 26.99254035949707
Loss: 27.176025390625
Loss: 28.210342407226562
Loss: 27.59347915649414
Loss: 27.092247009277344
Loss: 27.27893829345703
Loss: 26.350082397460938
Loss: 27.23853302001953
Loss: 27.216625213623047
Loss: 27.403528213500977
Loss: 27.132007598876953
Loss: 27.457910537719727
Loss: 26.274559020996094
Loss: 26.20831298828125
Loss: 26.470970153808594
Loss: 27.0873966217041
Loss: 26.721479415893555
Loss: 26.958499908447266
Loss: 27.335691452026367
Loss: 26.50365447998047
Loss: 27.220500946044922
Loss: 27.021759033203125
Loss: 27.149856567382812
[Train] Epoch 13, accuracy 0.8634548611111111
[Eval] Epoch 13, accuracy 0.840625
Model saved as x_small_model_weights_best.pth
Loss: 26.74639320373535
Loss: 26.516101837158203
Loss: 26.58334732055664
Loss: 26.750152587890625
Loss: 26.673446655273438
Loss: 25.521902084350586
Loss: 25.725887298583984
Loss: 26.240764617919922
Loss: 26.299074172973633
Loss: 25.683332443237305
Loss: 26.00832176208496
Loss: 26.313758850097656
Loss: 26.32577133178711
Loss: 27.360273361206055
Loss: 26.262296676635742
Loss: 25.830463409423828
Loss: 25.573768615722656
Loss: 25.485519409179688
Loss: 26.2313289642334
Loss: 26.265851974487305
Loss: 25.947124481201172
Loss: 26.054588317871094
Loss: 26.181503295898438
Loss: 26.4453125
Loss: 25.181455612182617
Loss: 25.85755157470703
Loss: 26.213315963745117
Loss: 26.248214721679688
Loss: 25.88506317138672
Loss: 25.893449783325195
Loss: 26.494384765625
Loss: 25.558862686157227
Loss: 25.95496940612793
Loss: 26.674488067626953
Loss: 26.157161712646484
Loss: 25.8231143951416
Loss: 25.712570190429688
Loss: 25.755292892456055
Loss: 25.38610076904297
Loss: 25.705602645874023
Loss: 26.34878158569336
Loss: 25.35674476623535
Loss: 25.192171096801758
Loss: 24.908315658569336
Loss: 25.390689849853516
Loss: 25.833242416381836
Loss: 26.128076553344727
Loss: 25.399288177490234
Loss: 25.210857391357422
Loss: 24.861440658569336
Loss: 25.81085968017578
Loss: 25.570655822753906
Loss: 25.110504150390625
Loss: 25.664888381958008
Loss: 25.56601333618164
Loss: 25.078630447387695
Loss: 25.61440658569336
Loss: 24.95779800415039
Loss: 25.139249801635742
Loss: 25.81790542602539
Loss: 25.17527961730957
Loss: 25.340911865234375
Loss: 25.723644256591797
Loss: 25.20796775817871
Loss: 25.171140670776367
Loss: 25.906248092651367
Loss: 24.730825424194336
Loss: 25.392742156982422
Loss: 25.353219985961914
Loss: 24.508981704711914
Loss: 24.492279052734375
Loss: 24.84801483154297
Loss: 24.610748291015625
Loss: 25.243228912353516
Loss: 25.334787368774414
Loss: 24.765316009521484
Loss: 24.223142623901367
Loss: 25.354639053344727
Loss: 25.1096134185791
Loss: 25.01417350769043
Loss: 25.909711837768555
Loss: 25.194982528686523
Loss: 25.560176849365234
Loss: 24.685094833374023
Loss: 24.99970245361328
Loss: 24.622093200683594
Loss: 25.287246704101562
Loss: 24.42444610595703
Loss: 23.726274490356445
Loss: 24.29625129699707
[Train] Epoch 14, accuracy 0.9085069444444445
[Eval] Epoch 14, accuracy 0.8708333333333333
Model saved as x_small_model_weights_best.pth
Loss: 24.820171356201172
Loss: 23.855649948120117
Loss: 23.869211196899414
Loss: 25.3023738861084
Loss: 24.79555892944336
Loss: 23.14504051208496
Loss: 24.457977294921875
Loss: 24.491321563720703
Loss: 24.096704483032227
Loss: 24.9146785736084
Loss: 23.437789916992188
Loss: 24.5068302154541
Loss: 24.40875816345215
Loss: 23.36804962158203
Loss: 24.29277229309082
Loss: 23.237730026245117
Loss: 23.580045700073242
Loss: 23.42452049255371
Loss: 23.367586135864258
Loss: 23.058731079101562
Loss: 23.28766441345215
Loss: 24.281864166259766
Loss: 24.179960250854492
Loss: 24.27526092529297
Loss: 23.368305206298828
Loss: 22.71653938293457
Loss: 24.06359100341797
Loss: 23.389720916748047
Loss: 23.94828987121582
Loss: 23.52178955078125
Loss: 23.992475509643555
Loss: 23.884281158447266
Loss: 23.19153594970703
Loss: 23.779468536376953
Loss: 23.1021728515625
Loss: 23.378387451171875
Loss: 22.67717742919922
Loss: 23.35911750793457
Loss: 23.27617645263672
Loss: 23.62513542175293
Loss: 23.845130920410156
Loss: 23.586326599121094
Loss: 23.05167579650879
Loss: 22.54102325439453
Loss: 23.220623016357422
Loss: 23.64463996887207
Loss: 23.686691284179688
Loss: 22.471155166625977
Loss: 23.289159774780273
Loss: 23.676498413085938
Loss: 24.187286376953125
Loss: 21.99441909790039
Loss: 23.14558219909668
Loss: 23.318761825561523
Loss: 22.504182815551758
Loss: 22.941417694091797
Loss: 23.16780662536621
Loss: 22.93555450439453
Loss: 23.01372528076172
Loss: 23.341419219970703
Loss: 22.986772537231445
Loss: 23.194974899291992
Loss: 22.851110458374023
Loss: 22.463197708129883
Loss: 23.92682647705078
Loss: 22.28363609313965
Loss: 22.932241439819336
Loss: 23.127323150634766
Loss: 23.36109733581543
Loss: 23.229440689086914
Loss: 23.33399200439453
Loss: 22.74372100830078
Loss: 22.695045471191406
Loss: 22.473262786865234
Loss: 23.03704261779785
Loss: 21.860811233520508
Loss: 23.431270599365234
Loss: 22.061342239379883
Loss: 23.405975341796875
Loss: 22.430959701538086
Loss: 22.27298927307129
Loss: 22.302431106567383
Loss: 21.881359100341797
Loss: 22.64981460571289
Loss: 22.571680068969727
Loss: 22.22626304626465
Loss: 22.475360870361328
Loss: 21.96147346496582
Loss: 22.12630271911621
Loss: 22.092891693115234
[Train] Epoch 15, accuracy 0.9328993055555556
[Eval] Epoch 15, accuracy 0.9253472222222222
Model saved as x_small_model_weights_best.pth
Loss: 22.97117805480957
Loss: 21.24526023864746
Loss: 21.968965530395508
Loss: 21.24156379699707
Loss: 21.686386108398438
Loss: 22.437744140625
Loss: 22.059017181396484
Loss: 20.52107048034668
Loss: 21.431903839111328
Loss: 21.80689811706543
Loss: 21.297117233276367
Loss: 22.612077713012695
Loss: 21.54313850402832
Loss: 22.339202880859375
Loss: 21.499290466308594
Loss: 22.01732635498047
Loss: 20.883949279785156
Loss: 21.12366485595703
Loss: 21.853206634521484
Loss: 21.103252410888672
Loss: 20.784500122070312
Loss: 20.864948272705078
Loss: 21.624412536621094
Loss: 20.30980682373047
Loss: 22.07776641845703
Loss: 20.328250885009766
Loss: 20.857736587524414
Loss: 22.174671173095703
Loss: 20.21656608581543
Loss: 21.331090927124023
Loss: 20.855228424072266
Loss: 21.472820281982422
Loss: 20.842769622802734
Loss: 20.366369247436523
Loss: 21.133100509643555
Loss: 21.231414794921875
Loss: 19.95479965209961
Loss: 21.71268653869629
Loss: 21.378137588500977
Loss: 20.53214454650879
Loss: 18.898717880249023
Loss: 21.5998592376709
Loss: 20.577390670776367
Loss: 20.183303833007812
Loss: 19.58777618408203
Loss: 21.511112213134766
Loss: 21.055025100708008
Loss: 20.676795959472656
Loss: 20.582439422607422
Loss: 19.56539535522461
Loss: 20.032926559448242
Loss: 21.105419158935547
Loss: 20.271587371826172
Loss: 20.986820220947266
Loss: 22.399961471557617
Loss: 19.921932220458984
Loss: 19.202417373657227
Loss: 21.086978912353516
Loss: 20.958837509155273
Loss: 20.218656539916992
Loss: 19.34189224243164
Loss: 19.639253616333008
Loss: 20.67381477355957
Loss: 19.75836181640625
Loss: 20.28167724609375
Loss: 20.083232879638672
Loss: 19.205900192260742
Loss: 19.48672866821289
Loss: 20.61157989501953
Loss: 19.278167724609375
Loss: 19.458251953125
Loss: 19.959081649780273
Loss: 20.263118743896484
Loss: 20.91438102722168
Loss: 20.60024642944336
Loss: 19.263166427612305
Loss: 19.26803207397461
Loss: 20.850013732910156
Loss: 20.327041625976562
Loss: 19.517166137695312
Loss: 19.638097763061523
Loss: 19.037181854248047
Loss: 18.836212158203125
Loss: 19.134756088256836
Loss: 19.25127410888672
Loss: 19.598041534423828
Loss: 19.923606872558594
Loss: 19.181856155395508
Loss: 19.66197395324707
Loss: 19.538362503051758
[Train] Epoch 16, accuracy 0.9510416666666667
[Eval] Epoch 16, accuracy 0.9364583333333333
Model saved as x_small_model_weights_best.pth
Loss: 19.48116111755371
Loss: 18.80660057067871
Loss: 18.268138885498047
Loss: 19.30902671813965
Loss: 18.38267707824707
Loss: 18.945724487304688
Loss: 18.733776092529297
Loss: 19.293563842773438
Loss: 20.079553604125977
Loss: 18.85028648376465
Loss: 19.24599266052246
Loss: 19.113201141357422
Loss: 18.32003402709961
Loss: 17.348255157470703
Loss: 17.965818405151367
Loss: 19.312877655029297
Loss: 19.542516708374023
Loss: 19.017322540283203
Loss: 19.798128128051758
Loss: 18.690622329711914
Loss: 19.533912658691406
Loss: 18.115066528320312
Loss: 18.26399803161621
Loss: 17.88648223876953
Loss: 18.890453338623047
Loss: 18.078025817871094
Loss: 19.02456283569336
Loss: 19.030235290527344
Loss: 18.521541595458984
Loss: 19.09500503540039
Loss: 18.858491897583008
Loss: 17.933225631713867
Loss: 18.713558197021484
Loss: 17.703344345092773
Loss: 17.941219329833984
Loss: 18.61248016357422
Loss: 18.27472496032715
Loss: 19.351850509643555
Loss: 18.426647186279297
Loss: 18.52161979675293
Loss: 18.498186111450195
Loss: 17.739242553710938
Loss: 18.23114776611328
Loss: 18.318225860595703
Loss: 18.13854217529297
Loss: 18.248464584350586
Loss: 18.168119430541992
Loss: 17.936601638793945
Loss: 18.671831130981445
Loss: 18.125469207763672
Loss: 17.8009090423584
Loss: 18.71343994140625
Loss: 18.05109977722168
Loss: 18.233551025390625
Loss: 19.376522064208984
Loss: 17.690204620361328
Loss: 17.141820907592773
Loss: 18.11963653564453
Loss: 17.439897537231445
Loss: 17.972679138183594
Loss: 18.06622886657715
Loss: 17.12653160095215
Loss: 19.02736473083496
Loss: 17.400638580322266
Loss: 17.50636863708496
Loss: 17.847692489624023
Loss: 17.063325881958008
Loss: 17.479480743408203
Loss: 17.37409782409668
Loss: 18.505624771118164
Loss: 18.42603874206543
Loss: 18.351947784423828
Loss: 18.405574798583984
Loss: 18.185047149658203
Loss: 17.228038787841797
Loss: 17.904794692993164
Loss: 17.643321990966797
Loss: 17.0776424407959
Loss: 18.35963249206543
Loss: 15.649065017700195
Loss: 16.999841690063477
Loss: 17.047822952270508
Loss: 17.238115310668945
Loss: 16.4121150970459
Loss: 16.656702041625977
Loss: 16.545608520507812
Loss: 16.826622009277344
Loss: 18.124658584594727
Loss: 18.005104064941406
Loss: 16.967693328857422
[Train] Epoch 17, accuracy 0.9645833333333333
[Eval] Epoch 17, accuracy 0.9482638888888889
Model saved as x_small_model_weights_best.pth
Loss: 17.09305763244629
Loss: 16.3980655670166
Loss: 17.712051391601562
Loss: 17.579387664794922
Loss: 15.571516990661621
Loss: 17.173648834228516
Loss: 16.03209114074707
Loss: 17.11773109436035
Loss: 16.19359016418457
Loss: 15.968629837036133
Loss: 16.039426803588867
Loss: 16.40894889831543
Loss: 16.812793731689453
Loss: 16.296619415283203
Loss: 15.544269561767578
Loss: 16.607608795166016
Loss: 16.56098175048828
Loss: 18.879371643066406
Loss: 16.544111251831055
Loss: 17.14885902404785
Loss: 16.054752349853516
Loss: 16.772584915161133
Loss: 15.645359992980957
Loss: 17.32037353515625
Loss: 17.0212345123291
Loss: 15.488893508911133
Loss: 16.458288192749023
Loss: 17.355493545532227
Loss: 16.734853744506836
Loss: 16.529434204101562
Loss: 15.224053382873535
Loss: 16.820226669311523
Loss: 15.190549850463867
Loss: 17.25086784362793
Loss: 15.815230369567871
Loss: 15.30908489227295
Loss: 16.48753547668457
Loss: 16.557756423950195
Loss: 15.331258773803711
Loss: 16.077592849731445
Loss: 16.353858947753906
Loss: 15.840083122253418
Loss: 15.28757381439209
Loss: 15.429466247558594
Loss: 15.681153297424316
Loss: 14.902029991149902
Loss: 16.272567749023438
Loss: 15.99952220916748
Loss: 16.241479873657227
Loss: 14.965819358825684
Loss: 14.518925666809082
Loss: 16.23748207092285
Loss: 14.546822547912598
Loss: 16.198793411254883
Loss: 15.092801094055176
Loss: 16.075443267822266
Loss: 15.848417282104492
Loss: 14.845389366149902
Loss: 14.835288047790527
Loss: 15.50767993927002
Loss: 13.79879379272461
Loss: 15.168440818786621
Loss: 15.426145553588867
Loss: 17.675792694091797
Loss: 15.00601577758789
Loss: 15.6084566116333
Loss: 16.123666763305664
Loss: 16.472951889038086
Loss: 16.392343521118164
Loss: 16.60110855102539
Loss: 16.57493019104004
Loss: 14.682143211364746
Loss: 16.04962730407715
Loss: 14.547422409057617
Loss: 16.286441802978516
Loss: 15.70966625213623
Loss: 15.223297119140625
Loss: 14.718327522277832
Loss: 15.29415512084961
Loss: 14.303519248962402
Loss: 15.217522621154785
Loss: 15.435701370239258
Loss: 15.058124542236328
Loss: 15.12216567993164
Loss: 15.623461723327637
Loss: 15.116720199584961
Loss: 14.967554092407227
Loss: 15.577703475952148
Loss: 15.560422897338867
Loss: 14.897185325622559
[Train] Epoch 18, accuracy 0.97109375
[Eval] Epoch 18, accuracy 0.9572916666666667
Model saved as x_small_model_weights_best.pth
Loss: 15.094267845153809
Loss: 13.913105964660645
Loss: 13.537217140197754
Loss: 14.086944580078125
Loss: 14.209738731384277
Loss: 13.985939025878906
Loss: 14.071939468383789
Loss: 14.282033920288086
Loss: 14.132740020751953
Loss: 13.890833854675293
Loss: 13.896647453308105
Loss: 13.398883819580078
Loss: 13.77238941192627
Loss: 13.514782905578613
Loss: 13.036526679992676
Loss: 15.042994499206543
Loss: 14.17619800567627
Loss: 14.675665855407715
Loss: 13.958086013793945
Loss: 14.39369010925293
Loss: 15.278388977050781
Loss: 15.30531120300293
Loss: 13.835712432861328
Loss: 14.53676700592041
Loss: 13.798327445983887
Loss: 13.962753295898438
Loss: 13.800108909606934
Loss: 13.59997272491455
Loss: 14.292447090148926
Loss: 14.696945190429688
Loss: 15.21774959564209
Loss: 14.700858116149902
Loss: 13.677352905273438
Loss: 12.088740348815918
Loss: 13.853429794311523
Loss: 13.958380699157715
Loss: 14.504584312438965
Loss: 14.271088600158691
Loss: 14.045980453491211
Loss: 12.613632202148438
Loss: 12.309528350830078
Loss: 11.673423767089844
Loss: 14.410055160522461
Loss: 13.058067321777344
Loss: 13.732970237731934
Loss: 13.72859001159668
Loss: 14.843611717224121
Loss: 13.84533405303955
Loss: 14.237752914428711
Loss: 13.515345573425293
Loss: 13.177336692810059
Loss: 12.60547161102295
Loss: 13.48159122467041
Loss: 12.953558921813965
Loss: 13.488020896911621
Loss: 12.882732391357422
Loss: 13.70427417755127
Loss: 14.162983894348145
Loss: 12.796788215637207
Loss: 13.969326972961426
Loss: 12.84708309173584
Loss: 13.151812553405762
Loss: 15.365805625915527
Loss: 12.819136619567871
Loss: 13.63714599609375
Loss: 12.205320358276367
Loss: 12.632966995239258
Loss: 14.010151863098145
Loss: 13.54228687286377
Loss: 13.401153564453125
Loss: 13.14582633972168
Loss: 13.25661563873291
Loss: 14.635570526123047
Loss: 14.040911674499512
Loss: 12.05747127532959
Loss: 12.501056671142578
Loss: 13.17340087890625
Loss: 11.816581726074219
Loss: 11.510459899902344
Loss: 13.26098346710205
Loss: 12.810925483703613
Loss: 12.075815200805664
Loss: 12.236650466918945
Loss: 12.671371459960938
Loss: 15.694694519042969
Loss: 12.68170166015625
Loss: 11.128103256225586
Loss: 12.060317039489746
Loss: 13.313421249389648
Loss: 12.24227523803711
[Train] Epoch 19, accuracy 0.9787326388888888
[Eval] Epoch 19, accuracy 0.9736111111111111
Model saved as x_small_model_weights_best.pth
Loss: 12.00911808013916
Loss: 12.35491943359375
Loss: 11.632871627807617
Loss: 10.677709579467773
Loss: 11.797137260437012
Loss: 11.525092124938965
Loss: 11.865212440490723
Loss: 11.693754196166992
Loss: 11.80140495300293
Loss: 11.702764511108398
Loss: 12.057374000549316
Loss: 11.925179481506348
Loss: 11.466209411621094
Loss: 12.425444602966309
Loss: 13.348211288452148
Loss: 12.037740707397461
Loss: 13.199064254760742
Loss: 12.603785514831543
Loss: 12.816673278808594
Loss: 11.358282089233398
Loss: 11.576950073242188
Loss: 12.753302574157715
Loss: 12.526139259338379
Loss: 12.449158668518066
Loss: 10.996184349060059
Loss: 11.53869342803955
Loss: 12.70075798034668
Loss: 13.820188522338867
Loss: 14.079126358032227
Loss: 12.05840015411377
Loss: 12.653794288635254
Loss: 12.519865036010742
Loss: 10.967432022094727
Loss: 12.024287223815918
Loss: 11.77598762512207
Loss: 11.325511932373047
Loss: 11.479496002197266
Loss: 13.164597511291504
Loss: 11.01272964477539
Loss: 11.454193115234375
Loss: 12.438419342041016
Loss: 10.161375045776367
Loss: 11.733450889587402
Loss: 11.614526748657227
Loss: 13.049858093261719
Loss: 12.446769714355469
Loss: 11.588014602661133
Loss: 11.890538215637207
Loss: 11.911323547363281
Loss: 11.907539367675781
Loss: 10.529985427856445
Loss: 11.896368026733398
Loss: 10.623906135559082
Loss: 12.166423797607422
Loss: 10.920632362365723
Loss: 10.092957496643066
Loss: 11.075634956359863
Loss: 10.444642066955566
Loss: 12.806647300720215
Loss: 11.386772155761719
Loss: 11.934849739074707
Loss: 11.411094665527344
Loss: 11.173322677612305
Loss: 12.437902450561523
Loss: 12.259736061096191
Loss: 11.317505836486816
Loss: 10.831005096435547
Loss: 12.274004936218262
Loss: 11.715852737426758
Loss: 11.035113334655762
Loss: 11.362052917480469
Loss: 9.650045394897461
Loss: 9.594775199890137
Loss: 10.982247352600098
Loss: 9.19431209564209
Loss: 11.800600051879883
Loss: 10.401128768920898
Loss: 10.468971252441406
Loss: 11.441356658935547
Loss: 10.907285690307617
Loss: 9.576005935668945
Loss: 12.12744426727295
Loss: 11.50365161895752
Loss: 11.626702308654785
Loss: 9.637171745300293
Loss: 11.369702339172363
Loss: 11.988139152526855
Loss: 12.038411140441895
Loss: 10.572735786437988
Loss: 11.465075492858887
[Train] Epoch 20, accuracy 0.9811631944444444
[Eval] Epoch 20, accuracy 0.9708333333333333
Loss: 10.308754920959473
Loss: 10.642240524291992
Loss: 10.52122688293457
Loss: 9.87601089477539
Loss: 10.905303955078125
Loss: 10.519966125488281
Loss: 11.20841121673584
Loss: 11.90749740600586
Loss: 10.372171401977539
Loss: 11.05822467803955
Loss: 9.746271133422852
Loss: 10.356826782226562
Loss: 10.503150939941406
Loss: 9.997187614440918
Loss: 11.814190864562988
Loss: 10.482505798339844
Loss: 10.351218223571777
Loss: 9.430420875549316
Loss: 10.420357704162598
Loss: 12.228370666503906
Loss: 10.240092277526855
Loss: 9.737767219543457
Loss: 10.042564392089844
Loss: 9.9689359664917
Loss: 9.670066833496094
Loss: 10.272499084472656
Loss: 9.828964233398438
Loss: 10.11618709564209
Loss: 10.196269989013672
Loss: 9.358426094055176
Loss: 10.753851890563965
Loss: 9.988669395446777
Loss: 9.43816089630127
Loss: 9.225484848022461
Loss: 8.909432411193848
Loss: 11.622760772705078
Loss: 10.185847282409668
Loss: 9.93209457397461
Loss: 9.251089096069336
Loss: 9.894290924072266
Loss: 10.787075996398926
Loss: 10.298226356506348
Loss: 10.619771003723145
Loss: 9.204716682434082
Loss: 9.505669593811035
Loss: 9.351654052734375
Loss: 9.949555397033691
Loss: 9.222381591796875
Loss: 10.559232711791992
Loss: 9.464417457580566
Loss: 8.697131156921387
Loss: 10.710823059082031
Loss: 10.832054138183594
Loss: 8.874021530151367
Loss: 10.12075138092041
Loss: 9.63533878326416
Loss: 10.246662139892578
Loss: 10.46895694732666
Loss: 11.131839752197266
Loss: 10.273123741149902
Loss: 9.93305778503418
Loss: 8.916497230529785
Loss: 9.384683609008789
Loss: 9.079054832458496
Loss: 9.517216682434082
Loss: 8.938946723937988
Loss: 8.884108543395996
Loss: 10.215630531311035
Loss: 8.29943561553955
Loss: 10.524438858032227
Loss: 9.268704414367676
Loss: 8.401640892028809
Loss: 9.829627990722656
Loss: 9.176630020141602
Loss: 10.048316955566406
Loss: 8.861748695373535
Loss: 8.764205932617188
Loss: 8.919672012329102
Loss: 9.179447174072266
Loss: 8.922369956970215
Loss: 8.73311996459961
Loss: 8.218300819396973
Loss: 9.65960693359375
Loss: 9.724328994750977
Loss: 10.072837829589844
Loss: 9.848326683044434
Loss: 9.354655265808105
Loss: 10.71157169342041
Loss: 10.459835052490234
Loss: 8.785821914672852
[Train] Epoch 21, accuracy 0.9833333333333333
[Eval] Epoch 21, accuracy 0.9756944444444444
Model saved as x_small_model_weights_best.pth
Loss: 8.834158897399902
Loss: 8.980259895324707
Loss: 8.659672737121582
Loss: 7.966675281524658
Loss: 7.387973308563232
Loss: 9.538125991821289
Loss: 7.436529636383057
Loss: 8.133767127990723
Loss: 8.29753303527832
Loss: 9.151909828186035
Loss: 9.578267097473145
Loss: 8.624823570251465
Loss: 9.003467559814453
Loss: 8.753325462341309
Loss: 8.743084907531738
Loss: 9.321758270263672
Loss: 8.88237476348877
Loss: 8.862061500549316
Loss: 8.682550430297852
Loss: 9.62498664855957
Loss: 9.571296691894531
Loss: 8.334356307983398
Loss: 9.193129539489746
Loss: 7.368202209472656
Loss: 8.488764762878418
Loss: 8.905975341796875
Loss: 8.058894157409668
Loss: 7.521121501922607
Loss: 8.632837295532227
Loss: 8.080240249633789
Loss: 9.70749282836914
Loss: 8.624473571777344
Loss: 9.206841468811035
Loss: 8.62466812133789
Loss: 8.087681770324707
Loss: 8.020870208740234
Loss: 9.028959274291992
Loss: 10.262564659118652
Loss: 8.912297248840332
Loss: 9.137170791625977
Loss: 8.506302833557129
Loss: 7.835944175720215
Loss: 9.663320541381836
Loss: 8.473735809326172
Loss: 7.560805797576904
Loss: 8.287774085998535
Loss: 7.5676984786987305
Loss: 7.965373992919922
Loss: 10.039079666137695
Loss: 7.2385149002075195
Loss: 7.66517448425293
Loss: 7.681240081787109
Loss: 8.909071922302246
Loss: 8.565408706665039
Loss: 7.409543514251709
Loss: 8.01131820678711
Loss: 7.885823726654053
Loss: 9.099693298339844
Loss: 8.084436416625977
Loss: 8.299741744995117
Loss: 8.553351402282715
Loss: 8.674105644226074
Loss: 8.388851165771484
Loss: 7.163580894470215
Loss: 8.06301498413086
Loss: 8.301520347595215
Loss: 8.415400505065918
Loss: 8.636134147644043
Loss: 7.880924701690674
Loss: 7.248325347900391
Loss: 9.119975090026855
Loss: 8.776849746704102
Loss: 7.685985565185547
Loss: 8.183457374572754
Loss: 7.578616619110107
Loss: 8.484461784362793
Loss: 8.410640716552734
Loss: 8.439009666442871
Loss: 9.215057373046875
Loss: 8.278069496154785
Loss: 7.933324337005615
Loss: 7.331424236297607
Loss: 9.636833190917969
Loss: 7.535740375518799
Loss: 8.098198890686035
Loss: 7.798336029052734
Loss: 7.8021345138549805
Loss: 8.418354988098145
Loss: 8.193934440612793
Loss: 8.134786605834961
[Train] Epoch 22, accuracy 0.9839409722222222
[Eval] Epoch 22, accuracy 0.9763888888888889
Model saved as x_small_model_weights_best.pth
Loss: 7.41646146774292
Loss: 8.072166442871094
Loss: 6.838149070739746
Loss: 8.543830871582031
Loss: 6.995873928070068
Loss: 7.3598856925964355
Loss: 8.989434242248535
Loss: 7.235457420349121
Loss: 7.872096061706543
Loss: 6.871658802032471
Loss: 6.516007423400879
Loss: 6.34788179397583
Loss: 8.028701782226562
Loss: 7.770754814147949
Loss: 7.1036577224731445
Loss: 6.8496599197387695
Loss: 7.331936359405518
Loss: 7.263650894165039
Loss: 7.517725467681885
Loss: 7.504755973815918
Loss: 6.701291561126709
Loss: 7.0030517578125
Loss: 7.752350807189941
Loss: 6.957996368408203
Loss: 7.920478820800781
Loss: 7.476096153259277
Loss: 7.651212692260742
Loss: 6.773179531097412
Loss: 6.317768096923828
Loss: 7.808890342712402
Loss: 7.166898250579834
Loss: 6.607911109924316
Loss: 7.215265274047852
Loss: 7.100833415985107
Loss: 7.972155570983887
Loss: 6.473188400268555
Loss: 7.692138671875
Loss: 6.227363586425781
Loss: 6.45585823059082
Loss: 6.999820709228516
Loss: 7.193049430847168
Loss: 7.586454391479492
Loss: 7.927767753601074
Loss: 6.0870842933654785
Loss: 8.121600151062012
Loss: 6.736863613128662
Loss: 7.52149772644043
Loss: 6.724452018737793
Loss: 5.877121925354004
Loss: 9.000650405883789
Loss: 7.950493335723877
Loss: 5.918449401855469
Loss: 7.272258758544922
Loss: 7.56925630569458
Loss: 7.130819320678711
Loss: 7.263257026672363
Loss: 8.380809783935547
Loss: 8.840214729309082
Loss: 7.693142890930176
Loss: 7.4108734130859375
Loss: 6.401163578033447
Loss: 5.768796920776367
Loss: 7.124274253845215
Loss: 7.814417362213135
Loss: 6.169913291931152
Loss: 7.2140607833862305
Loss: 5.743799209594727
Loss: 7.780313968658447
Loss: 7.840478897094727
Loss: 7.036241054534912
Loss: 6.820155620574951
Loss: 7.8827948570251465
Loss: 6.864630222320557
Loss: 7.342127799987793
Loss: 6.823760032653809
Loss: 7.363956451416016
Loss: 6.795012950897217
Loss: 7.209288120269775
Loss: 5.750416278839111
Loss: 6.774223327636719
Loss: 5.761136531829834
Loss: 7.126759052276611
Loss: 6.393176078796387
Loss: 6.822390556335449
Loss: 5.479502201080322
Loss: 6.80178165435791
Loss: 6.823530197143555
Loss: 6.347414970397949
Loss: 7.051858901977539
Loss: 6.595837593078613
[Train] Epoch 23, accuracy 0.9860243055555555
[Eval] Epoch 23, accuracy 0.9819444444444444
Model saved as x_small_model_weights_best.pth
Loss: 5.955615520477295
Loss: 8.218343734741211
Loss: 6.961997985839844
Loss: 5.969545364379883
Loss: 6.496565818786621
Loss: 6.297513484954834
Loss: 6.301258087158203
Loss: 6.795129776000977
Loss: 8.260530471801758
Loss: 6.5846943855285645
Loss: 6.0316853523254395
Loss: 6.018043518066406
Loss: 7.062969207763672
Loss: 7.465799808502197
Loss: 6.906764507293701
Loss: 5.886672019958496
Loss: 6.848783493041992
Loss: 6.28975772857666
Loss: 5.485029697418213
Loss: 6.565906524658203
Loss: 7.472903728485107
Loss: 5.75253438949585
Loss: 6.996053695678711
Loss: 6.212899208068848
Loss: 7.835712432861328
Loss: 6.107501983642578
Loss: 6.455722808837891
Loss: 7.444476127624512
Loss: 5.952620029449463
Loss: 5.561404228210449
Loss: 6.308696269989014
Loss: 6.419559001922607
Loss: 5.774178981781006
Loss: 6.383486747741699
Loss: 6.9579176902771
Loss: 5.89901065826416
Loss: 5.499456405639648
Loss: 7.2216691970825195
Loss: 5.5642924308776855
Loss: 6.811671257019043
Loss: 5.995312213897705
Loss: 6.830815315246582
Loss: 5.80563497543335
Loss: 5.758868217468262
Loss: 6.563642501831055
Loss: 5.603015899658203
Loss: 6.701858043670654
Loss: 6.0596842765808105
Loss: 6.6750569343566895
Loss: 5.6672563552856445
Loss: 6.1069793701171875
Loss: 6.194097995758057
Loss: 5.270451545715332
Loss: 6.31535005569458
Loss: 6.612403869628906
Loss: 6.588367938995361
Loss: 5.906339168548584
Loss: 5.932436466217041
Loss: 6.014477252960205
Loss: 6.610071659088135
Loss: 5.583904266357422
Loss: 5.8017730712890625
Loss: 5.940715789794922
Loss: 6.335265159606934
Loss: 5.869118690490723
Loss: 6.345768928527832
Loss: 6.487912654876709
Loss: 5.86617374420166
Loss: 5.063086032867432
Loss: 5.302465438842773
Loss: 6.130801677703857
Loss: 6.163122653961182
Loss: 5.724033355712891
Loss: 6.322109222412109
Loss: 7.186581611633301
Loss: 6.085733890533447
Loss: 5.487488269805908
Loss: 6.132169723510742
Loss: 5.651182651519775
Loss: 5.623459815979004
Loss: 5.55682897567749
Loss: 5.379597187042236
Loss: 6.050066947937012
Loss: 6.594869136810303
Loss: 5.187325477600098
Loss: 5.035172939300537
Loss: 5.401088714599609
Loss: 4.987520694732666
Loss: 5.606114387512207
Loss: 5.909745693206787
[Train] Epoch 24, accuracy 0.9861111111111112
[Eval] Epoch 24, accuracy 0.9840277777777777
Model saved as x_small_model_weights_best.pth
Loss: 4.869704723358154
Loss: 5.0315399169921875
Loss: 5.964100360870361
Loss: 6.398509502410889
Loss: 5.677158355712891
Loss: 5.144170761108398
Loss: 3.918544292449951
Loss: 4.838826656341553
Loss: 5.816189765930176
Loss: 4.687259674072266
Loss: 5.92399787902832
Loss: 5.546719551086426
Loss: 4.945274829864502
Loss: 4.6602864265441895
Loss: 5.181432723999023
Loss: 5.525320053100586
Loss: 5.136524677276611
Loss: 4.856815338134766
Loss: 5.985655307769775
Loss: 6.578602313995361
Loss: 6.446314334869385
Loss: 5.07156229019165
Loss: 5.82037878036499
Loss: 5.819409370422363
Loss: 5.254101276397705
Loss: 6.976225852966309
Loss: 5.69105339050293
Loss: 6.008100986480713
Loss: 5.293468475341797
Loss: 6.389516830444336
Loss: 6.387277603149414
Loss: 5.3300862312316895
Loss: 5.876592636108398
Loss: 5.653212070465088
Loss: 5.983198165893555
Loss: 5.528724193572998
Loss: 5.738005638122559
Loss: 5.6397318840026855
Loss: 5.932277202606201
Loss: 6.630814075469971
Loss: 7.085817337036133
Loss: 4.760397434234619
Loss: 6.567330360412598
Loss: 4.930343151092529
Loss: 5.667436122894287
Loss: 4.51809024810791
Loss: 5.292376518249512
Loss: 6.404387950897217
Loss: 4.8070387840271
Loss: 4.429516792297363
Loss: 6.701223850250244
Loss: 5.5875420570373535
Loss: 5.463803768157959
Loss: 5.02231502532959
Loss: 5.798644542694092
Loss: 5.25337028503418
Loss: 5.053614139556885
Loss: 5.179410934448242
Loss: 5.444587707519531
Loss: 5.818423271179199
Loss: 4.2078962326049805
Loss: 5.405607223510742
Loss: 5.339822292327881
Loss: 6.362863063812256
Loss: 5.998947620391846
Loss: 5.201844692230225
Loss: 4.535353183746338
Loss: 5.408170223236084
Loss: 6.063272476196289
Loss: 5.7383012771606445
Loss: 5.865582466125488
Loss: 5.499138355255127
Loss: 4.957973957061768
Loss: 6.581675052642822
Loss: 5.266417503356934
Loss: 5.569082260131836
Loss: 5.683760166168213
Loss: 4.987926959991455
Loss: 5.750575542449951
Loss: 4.818810939788818
Loss: 5.379583358764648
Loss: 5.371766567230225
Loss: 4.264336109161377
Loss: 4.641626358032227
Loss: 4.7543840408325195
Loss: 5.627535343170166
Loss: 4.918202877044678
Loss: 4.7958760261535645
Loss: 5.818203926086426
Loss: 5.545651912689209
[Train] Epoch 25, accuracy 0.9875868055555556
[Eval] Epoch 25, accuracy 0.9899305555555555
Model saved as x_small_model_weights_best.pth
Loss: 5.762868404388428
Loss: 5.008376598358154
Loss: 6.035848617553711
Loss: 5.530717372894287
Loss: 4.9138898849487305
Loss: 5.269857406616211
Loss: 4.267032146453857
Loss: 4.320971965789795
Loss: 5.500548839569092
Loss: 4.321518898010254
Loss: 6.264632701873779
Loss: 4.349624156951904
Loss: 4.554792404174805
Loss: 4.770459175109863
Loss: 4.6802263259887695
Loss: 5.356389045715332
Loss: 4.45883846282959
Loss: 5.359416484832764
Loss: 4.301031589508057
Loss: 6.275678634643555
Loss: 4.592413902282715
Loss: 5.6753034591674805
Loss: 5.22654914855957
Loss: 4.816669940948486
Loss: 5.343544006347656
Loss: 5.7711029052734375
Loss: 4.881937026977539
Loss: 5.750254154205322
Loss: 4.9442243576049805
Loss: 4.716634273529053
Loss: 5.704198837280273
Loss: 4.69964599609375
Loss: 4.8224992752075195
Loss: 4.665482997894287
Loss: 4.725680828094482
Loss: 5.997621536254883
Loss: 5.639734268188477
Loss: 4.294013023376465
Loss: 4.82517147064209
Loss: 5.228790283203125
Loss: 4.970888614654541
Loss: 4.603673458099365
Loss: 3.9706051349639893
Loss: 5.312829494476318
Loss: 5.225092887878418
Loss: 4.741568088531494
Loss: 5.209650039672852
Loss: 4.47404146194458
Loss: 4.548196315765381
Loss: 5.3278961181640625
Loss: 3.430790662765503
Loss: 4.4767303466796875
Loss: 4.633969306945801
Loss: 4.796238899230957
Loss: 5.227292537689209
Loss: 4.694879055023193
Loss: 4.638209342956543
Loss: 3.843916654586792
Loss: 4.261667728424072
Loss: 4.870579719543457
Loss: 4.21533727645874
Loss: 5.085905075073242
Loss: 5.073930263519287
Loss: 5.441195011138916
Loss: 4.420174598693848
Loss: 4.890535831451416
Loss: 4.766938209533691
Loss: 4.570818901062012
Loss: 4.19256067276001
Loss: 4.202114582061768
Loss: 4.957635402679443
Loss: 4.833241939544678
Loss: 4.909308433532715
Loss: 3.7645256519317627
Loss: 5.007071495056152
Loss: 5.139427185058594
Loss: 4.079474925994873
Loss: 3.7013700008392334
Loss: 4.658650875091553
Loss: 3.780484914779663
Loss: 3.773998975753784
Loss: 4.072294235229492
Loss: 4.062803268432617
Loss: 4.199526786804199
Loss: 5.484158515930176
Loss: 4.319594383239746
Loss: 3.982109308242798
Loss: 4.551570415496826
Loss: 5.379840850830078
Loss: 5.580603122711182
[Train] Epoch 26, accuracy 0.9881076388888889
[Eval] Epoch 26, accuracy 0.9864583333333333
Loss: 4.447327136993408
Loss: 4.9417266845703125
Loss: 3.557668924331665
Loss: 4.522972583770752
Loss: 3.7578327655792236
Loss: 4.733732223510742
Loss: 5.241086959838867
Loss: 4.070315837860107
Loss: 3.892378091812134
Loss: 4.915137767791748
Loss: 4.37168025970459
Loss: 4.285937309265137
Loss: 5.3439249992370605
Loss: 4.084357738494873
Loss: 3.9914839267730713
Loss: 4.352087020874023
Loss: 4.486519813537598
Loss: 3.820180892944336
Loss: 4.06327486038208
Loss: 3.703511953353882
Loss: 3.554384469985962
Loss: 4.212314605712891
Loss: 3.9794206619262695
Loss: 4.351486682891846
Loss: 3.9969115257263184
Loss: 4.897782325744629
Loss: 4.123131275177002
Loss: 4.133666038513184
Loss: 4.554829120635986
Loss: 4.27880334854126
Loss: 3.9255478382110596
Loss: 4.958631992340088
Loss: 4.809208869934082
Loss: 4.000400543212891
Loss: 4.457266807556152
Loss: 4.631410598754883
Loss: 4.342303276062012
Loss: 5.013857841491699
Loss: 4.3580145835876465
Loss: 4.201162338256836
Loss: 4.153162479400635
Loss: 4.558380126953125
Loss: 4.622657299041748
Loss: 3.543395519256592
Loss: 4.50556755065918
Loss: 4.110621452331543
Loss: 3.947035551071167
Loss: 5.454721927642822
Loss: 4.069975852966309
Loss: 3.6929397583007812
Loss: 5.667451858520508
Loss: 4.725312232971191
Loss: 5.1706366539001465
Loss: 3.8424553871154785
Loss: 5.087636947631836
Loss: 4.11109733581543
Loss: 4.6799492835998535
Loss: 3.14939546585083
Loss: 4.037942409515381
Loss: 4.044670581817627
Loss: 4.631937026977539
Loss: 4.894006252288818
Loss: 3.8985297679901123
Loss: 4.972949028015137
Loss: 5.095550537109375
Loss: 4.263421535491943
Loss: 2.917459487915039
Loss: 4.517075061798096
Loss: 3.8011839389801025
Loss: 5.0582475662231445
Loss: 4.433012008666992
Loss: 4.384187698364258
Loss: 4.317892074584961
Loss: 3.690932035446167
Loss: 3.510326623916626
Loss: 4.1873698234558105
Loss: 3.9116592407226562
Loss: 4.424195289611816
Loss: 4.573326587677002
Loss: 4.9185051918029785
Loss: 3.5299220085144043
Loss: 3.9686551094055176
Loss: 3.9280900955200195
Loss: 3.660597324371338
Loss: 3.462235450744629
Loss: 4.548178195953369
Loss: 3.757995843887329
Loss: 4.659399032592773
Loss: 4.339616298675537
Loss: 4.486839294433594
[Train] Epoch 27, accuracy 0.9893229166666667
[Eval] Epoch 27, accuracy 0.9909722222222223
Model saved as x_small_model_weights_best.pth
Loss: 3.727966547012329
Loss: 3.3890068531036377
Loss: 3.314556360244751
Loss: 2.943317174911499
Loss: 3.5625622272491455
Loss: 4.465764045715332
Loss: 4.0913405418396
Loss: 3.810220241546631
Loss: 3.6759512424468994
Loss: 4.194276809692383
Loss: 3.5656371116638184
Loss: 3.9026548862457275
Loss: 3.3311264514923096
Loss: 4.175875663757324
Loss: 3.386121988296509
Loss: 4.492899417877197
Loss: 3.6697428226470947
Loss: 4.760100364685059
Loss: 3.0898914337158203
Loss: 4.206648349761963
Loss: 4.515381813049316
Loss: 3.309638500213623
Loss: 4.353879928588867
Loss: 3.840672731399536
Loss: 3.965676784515381
Loss: 3.7623343467712402
Loss: 3.6161787509918213
Loss: 4.644542694091797
Loss: 3.6487538814544678
Loss: 3.678760051727295
Loss: 4.020818710327148
Loss: 4.628482341766357
Loss: 4.423189163208008
Loss: 3.837395668029785
Loss: 2.553157091140747
Loss: 3.8635308742523193
Loss: 3.6294937133789062
Loss: 3.9832634925842285
Loss: 4.26784086227417
Loss: 3.2022666931152344
Loss: 4.308903694152832
Loss: 3.54650616645813
Loss: 4.262688159942627
Loss: 3.774789333343506
Loss: 3.25909161567688
Loss: 3.805530548095703
Loss: 4.551739692687988
Loss: 3.2146458625793457
Loss: 4.131044864654541
Loss: 3.5462989807128906
Loss: 3.505328416824341
Loss: 4.236352920532227
Loss: 3.7909064292907715
Loss: 4.3387370109558105
Loss: 3.4365041255950928
Loss: 3.504659414291382
Loss: 4.05383825302124
Loss: 3.949486494064331
Loss: 3.5764334201812744
Loss: 4.214641094207764
Loss: 3.9465997219085693
Loss: 4.164218902587891
Loss: 4.2348856925964355
Loss: 5.052291393280029
Loss: 3.7781496047973633
Loss: 4.696001052856445
Loss: 3.584101915359497
Loss: 4.182854175567627
Loss: 2.931293249130249
Loss: 3.958897113800049
Loss: 3.945322036743164
Loss: 3.1194310188293457
Loss: 3.853039026260376
Loss: 3.675325632095337
Loss: 3.6371498107910156
Loss: 3.615558624267578
Loss: 3.189155101776123
Loss: 3.6212964057922363
Loss: 3.3963215351104736
Loss: 3.97922945022583
Loss: 2.6303975582122803
Loss: 3.5798418521881104
Loss: 3.8325817584991455
Loss: 3.1119401454925537
Loss: 4.177295684814453
Loss: 3.730179786682129
Loss: 3.5375871658325195
Loss: 3.476006031036377
Loss: 3.966398239135742
Loss: 5.058013439178467
[Train] Epoch 28, accuracy 0.9901909722222222
[Eval] Epoch 28, accuracy 0.9868055555555556
Loss: 3.845966100692749
Loss: 3.0685322284698486
Loss: 3.193988561630249
Loss: 3.2900094985961914
Loss: 2.9344208240509033
Loss: 3.392859935760498
Loss: 3.689727306365967
Loss: 3.532783269882202
Loss: 2.8495218753814697
Loss: 3.841585159301758
Loss: 3.080641269683838
Loss: 4.192392349243164
Loss: 3.4092445373535156
Loss: 2.346407413482666
Loss: 3.7878952026367188
Loss: 2.7804388999938965
Loss: 2.7716450691223145
Loss: 2.8876397609710693
Loss: 3.3177871704101562
Loss: 3.509627342224121
Loss: 3.733150005340576
Loss: 3.3950226306915283
Loss: 3.4134035110473633
Loss: 2.8249645233154297
Loss: 3.097921371459961
Loss: 3.1451151371002197
Loss: 3.4826581478118896
Loss: 3.4078943729400635
Loss: 3.505202293395996
Loss: 2.940824508666992
Loss: 3.4288463592529297
Loss: 3.6023354530334473
Loss: 3.239429473876953
Loss: 4.482831001281738
Loss: 3.2992074489593506
Loss: 2.9918949604034424
Loss: 3.681811809539795
Loss: 3.543583631515503
Loss: 4.071979999542236
Loss: 2.138941764831543
Loss: 2.520137310028076
Loss: 2.8443899154663086
Loss: 3.1589999198913574
Loss: 3.487355947494507
Loss: 3.790936231613159
Loss: 4.021256923675537
Loss: 3.320821523666382
Loss: 3.406522512435913
Loss: 3.027693271636963
Loss: 3.1580708026885986
Loss: 3.3161110877990723
Loss: 2.979630947113037
Loss: 3.525207996368408
Loss: 3.474060535430908
Loss: 3.9254820346832275
Loss: 3.3934719562530518
Loss: 3.8073089122772217
Loss: 3.9283242225646973
Loss: 3.1679608821868896
Loss: 2.885737895965576
Loss: 3.3756215572357178
Loss: 3.1556472778320312
Loss: 3.4011471271514893
Loss: 2.784888744354248
Loss: 4.103912830352783
Loss: 3.3414173126220703
Loss: 3.542750597000122
Loss: 2.929239273071289
Loss: 3.7220568656921387
Loss: 3.485461711883545
Loss: 4.8392839431762695
Loss: 2.808866262435913
Loss: 3.3366949558258057
Loss: 3.09594988822937
Loss: 2.3323750495910645
Loss: 3.1818504333496094
Loss: 3.0284409523010254
Loss: 4.017067909240723
Loss: 3.100921392440796
Loss: 3.825317859649658
Loss: 2.970475912094116
Loss: 3.319366693496704
Loss: 3.5207717418670654
Loss: 3.2879719734191895
Loss: 2.7089977264404297
Loss: 3.6883790493011475
Loss: 3.351578950881958
Loss: 2.749447822570801
Loss: 3.4503610134124756
Loss: 3.4590938091278076
[Train] Epoch 29, accuracy 0.9912326388888889
[Eval] Epoch 29, accuracy 0.9909722222222223
Loss: 2.913525104522705
Loss: 3.1803455352783203
Loss: 3.8995368480682373
Loss: 2.666391372680664
Loss: 2.711714744567871
Loss: 3.09199595451355
Loss: 2.704146146774292
Loss: 3.065455436706543
Loss: 3.1864876747131348
Loss: 3.4771063327789307
Loss: 2.7204651832580566
Loss: 3.1706349849700928
Loss: 5.045393466949463
Loss: 2.6577365398406982
Loss: 3.3378894329071045
Loss: 2.4905874729156494
Loss: 3.245285749435425
Loss: 3.090050458908081
Loss: 3.1453022956848145
Loss: 3.4189579486846924
Loss: 3.002619743347168
Loss: 2.935938835144043
Loss: 2.656013011932373
Loss: 3.3966169357299805
Loss: 2.9666061401367188
Loss: 3.184614658355713
Loss: 3.520826816558838
Loss: 3.243102550506592
Loss: 2.6656832695007324
Loss: 2.8871166706085205
Loss: 4.0719194412231445
Loss: 3.1086537837982178
Loss: 2.666898727416992
Loss: 3.1175360679626465
Loss: 3.295083999633789
Loss: 4.041227340698242
Loss: 3.098874568939209
Loss: 3.3883159160614014
Loss: 3.155932664871216
Loss: 2.8624982833862305
Loss: 2.528104782104492
Loss: 3.159912109375
Loss: 2.5997073650360107
Loss: 3.5755062103271484
Loss: 3.1756534576416016
Loss: 2.214179754257202
Loss: 3.022949457168579
Loss: 3.439138412475586
Loss: 2.630894184112549
Loss: 2.8208351135253906
Loss: 2.6616525650024414
Loss: 3.1209774017333984
Loss: 3.3125808238983154
Loss: 3.2547740936279297
Loss: 3.5076725482940674
Loss: 2.8170557022094727
Loss: 3.075082778930664
Loss: 3.0469512939453125
Loss: 3.1097686290740967
Loss: 2.7217159271240234
Loss: 2.7807114124298096
Loss: 3.066197156906128
Loss: 3.0481998920440674
Loss: 2.4496893882751465
Loss: 3.5170812606811523
Loss: 3.325650691986084
Loss: 3.5716638565063477
Loss: 2.4800453186035156
Loss: 3.5482537746429443
Loss: 3.365537166595459
Loss: 3.76237416267395
Loss: 2.7630527019500732
Loss: 3.1005139350891113
Loss: 3.6206305027008057
Loss: 2.6159884929656982
Loss: 3.4382638931274414
Loss: 3.5133373737335205
Loss: 2.888434648513794
Loss: 3.04692006111145
Loss: 2.9871926307678223
Loss: 2.5242350101470947
Loss: 3.28288197517395
Loss: 2.968327283859253
Loss: 2.8549466133117676
Loss: 3.071615219116211
Loss: 3.752472400665283
Loss: 2.637524127960205
Loss: 2.3460397720336914
Loss: 3.521404981613159
Loss: 3.991737127304077
[Train] Epoch 30, accuracy 0.9912326388888889
[Eval] Epoch 30, accuracy 0.9927083333333333
Model saved as x_small_model_weights_best.pth
Loss: 3.609464168548584
Loss: 2.642314910888672
Loss: 3.478095293045044
Loss: 2.114438056945801
Loss: 1.9684863090515137
Loss: 2.832712411880493
Loss: 2.4499740600585938
Loss: 2.727879285812378
Loss: 3.0165278911590576
Loss: 2.5783050060272217
Loss: 2.8197662830352783
Loss: 4.390322685241699
Loss: 2.5974538326263428
Loss: 2.975790500640869
Loss: 2.600421905517578
Loss: 3.165820360183716
Loss: 2.4711625576019287
Loss: 3.0770275592803955
Loss: 2.7806944847106934
Loss: 3.2242462635040283
Loss: 2.859174966812134
Loss: 2.9519705772399902
Loss: 3.2182717323303223
Loss: 1.9809603691101074
Loss: 3.183993339538574
Loss: 3.415039539337158
Loss: 3.082606554031372
Loss: 3.2151589393615723
Loss: 3.040750503540039
Loss: 3.3746142387390137
Loss: 2.600735902786255
Loss: 3.364777088165283
Loss: 2.6616363525390625
Loss: 3.2542612552642822
Loss: 3.43304443359375
Loss: 2.5150792598724365
Loss: 2.5841562747955322
Loss: 2.470881462097168
Loss: 3.4744491577148438
Loss: 3.043313503265381
Loss: 2.545945882797241
Loss: 2.750293016433716
Loss: 3.0713846683502197
Loss: 3.2316370010375977
Loss: 2.7425901889801025
Loss: 3.1992239952087402
Loss: 2.009645938873291
Loss: 4.126692771911621
Loss: 3.3895535469055176
Loss: 3.9262547492980957
Loss: 2.768817901611328
Loss: 2.730207681655884
Loss: 3.3477694988250732
Loss: 3.1302685737609863
Loss: 2.496015787124634
Loss: 3.4132137298583984
Loss: 3.2412450313568115
Loss: 3.3241851329803467
Loss: 2.851956367492676
Loss: 2.7823593616485596
Loss: 3.0886526107788086
Loss: 2.798513412475586
Loss: 2.861032724380493
Loss: 2.357630491256714
Loss: 2.238337278366089
Loss: 3.079409599304199
Loss: 2.30696177482605
Loss: 2.159923553466797
Loss: 3.2047786712646484
Loss: 2.4904329776763916
Loss: 2.5876638889312744
Loss: 3.1433889865875244
Loss: 2.6152267456054688
Loss: 3.0122835636138916
Loss: 2.378706693649292
Loss: 3.6595168113708496
Loss: 3.6452901363372803
Loss: 2.907106637954712
Loss: 3.2885196208953857
Loss: 3.493406057357788
Loss: 2.8437047004699707
Loss: 2.7863850593566895
Loss: 2.070648670196533
Loss: 3.206576347351074
Loss: 2.31821608543396
Loss: 1.9396809339523315
Loss: 2.8104066848754883
Loss: 3.0355746746063232
Loss: 2.5418577194213867
Loss: 3.033203601837158
[Train] Epoch 31, accuracy 0.9915798611111111
[Eval] Epoch 31, accuracy 0.9899305555555555
Loss: 2.721447706222534
Loss: 1.7451943159103394
Loss: 2.1796116828918457
Loss: 2.574326276779175
Loss: 3.1681458950042725
Loss: 3.285078525543213
Loss: 2.246772050857544
Loss: 2.877357006072998
Loss: 2.736823797225952
Loss: 3.212576389312744
Loss: 2.678439140319824
Loss: 2.767425775527954
Loss: 2.5923807621002197
Loss: 2.439718723297119
Loss: 3.2458248138427734
Loss: 1.814112901687622
Loss: 2.24222993850708
Loss: 2.3677425384521484
Loss: 2.6265199184417725
Loss: 2.309943914413452
Loss: 2.405001401901245
Loss: 3.1053037643432617
Loss: 2.6073029041290283
Loss: 2.4027609825134277
Loss: 2.2193734645843506
Loss: 2.1494274139404297
Loss: 3.909255266189575
Loss: 2.6065351963043213
Loss: 2.514726400375366
Loss: 3.0293209552764893
Loss: 2.4767165184020996
Loss: 2.6171350479125977
Loss: 3.5696394443511963
Loss: 2.804306983947754
Loss: 2.659532308578491
Loss: 3.103245735168457
Loss: 2.5910592079162598
Loss: 2.3865926265716553
Loss: 2.8866629600524902
Loss: 2.8512160778045654
Loss: 3.3339364528656006
Loss: 3.0192935466766357
Loss: 2.8304905891418457
Loss: 2.7326693534851074
Loss: 2.9397244453430176
Loss: 2.9021921157836914
Loss: 2.7803311347961426
Loss: 3.4383010864257812
Loss: 3.1991841793060303
Loss: 2.388059377670288
Loss: 3.2540369033813477
Loss: 2.485663652420044
Loss: 2.7415473461151123
Loss: 2.0455269813537598
Loss: 2.429830551147461
Loss: 1.8644603490829468
Loss: 2.516105890274048
Loss: 2.5435585975646973
Loss: 3.1388792991638184
Loss: 3.3416950702667236
Loss: 3.346599578857422
Loss: 1.696030855178833
Loss: 2.295886754989624
Loss: 2.47271990776062
Loss: 2.56569242477417
Loss: 2.950869083404541
Loss: 3.151477098464966
Loss: 2.434309482574463
Loss: 3.2128889560699463
Loss: 3.0013186931610107
Loss: 3.093251943588257
Loss: 2.4281163215637207
Loss: 2.9823789596557617
Loss: 3.8274381160736084
Loss: 2.654547691345215
Loss: 2.103595495223999
Loss: 3.1230521202087402
Loss: 2.389716625213623
Loss: 3.0110700130462646
Loss: 3.199155807495117
Loss: 2.439692735671997
Loss: 1.723840355873108
Loss: 2.15969181060791
Loss: 2.4748311042785645
Loss: 2.357581615447998
Loss: 3.2317585945129395
Loss: 2.525472402572632
Loss: 2.8824949264526367
Loss: 1.7023411989212036
Loss: 1.888182520866394
[Train] Epoch 32, accuracy 0.9921875
[Eval] Epoch 32, accuracy 0.9930555555555556
Model saved as x_small_model_weights_best.pth
Loss: 2.5657942295074463
Loss: 2.7034502029418945
Loss: 2.464095115661621
Loss: 2.0076775550842285
Loss: 2.9680585861206055
Loss: 2.4010183811187744
Loss: 2.2856507301330566
Loss: 2.7239458560943604
Loss: 2.1328229904174805
Loss: 1.9072282314300537
Loss: 3.036067247390747
Loss: 1.8058867454528809
Loss: 3.1508238315582275
Loss: 2.8888099193573
Loss: 2.4980642795562744
Loss: 2.3173444271087646
Loss: 1.8855570554733276
Loss: 2.492396831512451
Loss: 3.6997733116149902
Loss: 2.4191877841949463
Loss: 2.720150947570801
Loss: 3.262547016143799
Loss: 3.8605737686157227
Loss: 3.655013084411621
Loss: 3.9473624229431152
Loss: 2.50752854347229
Loss: 2.722327709197998
Loss: 2.4625372886657715
Loss: 3.0066030025482178
Loss: 2.5430374145507812
Loss: 3.1402080059051514
Loss: 2.868807315826416
Loss: 2.501066207885742
Loss: 2.340264320373535
Loss: 2.8286538124084473
Loss: 3.0055348873138428
Loss: 2.6718995571136475
Loss: 2.5079662799835205
Loss: 1.9851086139678955
Loss: 2.018425941467285
Loss: 2.700293779373169
Loss: 2.544339656829834
Loss: 2.5504562854766846
Loss: 2.848193645477295
Loss: 3.278658390045166
Loss: 1.7520790100097656
Loss: 2.7667391300201416
Loss: 2.529719352722168
Loss: 2.120187282562256
Loss: 2.4004952907562256
Loss: 2.325364828109741
Loss: 1.844496488571167
Loss: 2.4256017208099365
Loss: 2.117652654647827
Loss: 3.0128555297851562
Loss: 2.8524599075317383
Loss: 2.01094126701355
Loss: 2.319605827331543
Loss: 1.9328510761260986
Loss: 2.7938408851623535
Loss: 2.69002628326416
Loss: 3.2732341289520264
Loss: 2.221346855163574
Loss: 2.3688385486602783
Loss: 2.148996114730835
Loss: 2.5739567279815674
Loss: 2.8014285564422607
Loss: 2.639960765838623
Loss: 3.0117805004119873
Loss: 2.2719106674194336
Loss: 2.8888282775878906
Loss: 1.9245160818099976
Loss: 2.415250539779663
Loss: 2.4514663219451904
Loss: 3.5463407039642334
Loss: 1.8142775297164917
Loss: 2.329746961593628
Loss: 2.535548210144043
Loss: 2.2978241443634033
Loss: 3.269388437271118
Loss: 2.912902355194092
Loss: 2.584406852722168
Loss: 1.9439677000045776
Loss: 2.8693721294403076
Loss: 2.33109188079834
Loss: 1.9928371906280518
Loss: 2.5798964500427246
Loss: 2.4937565326690674
Loss: 3.2462377548217773
Loss: 2.937042236328125
[Train] Epoch 33, accuracy 0.9912326388888889
[Eval] Epoch 33, accuracy 0.9923611111111111
Loss: 2.83439302444458
Loss: 2.3780722618103027
Loss: 2.356588363647461
Loss: 1.7802543640136719
Loss: 1.8995033502578735
Loss: 2.001568078994751
Loss: 1.8236230611801147
Loss: 2.3889083862304688
Loss: 2.319303035736084
Loss: 2.102433443069458
Loss: 1.9697545766830444
Loss: 3.2619788646698
Loss: 2.684513807296753
Loss: 2.356069803237915
Loss: 2.8617324829101562
Loss: 2.424407720565796
Loss: 1.839646339416504
Loss: 2.380509853363037
Loss: 2.7982938289642334
Loss: 2.0469260215759277
Loss: 2.113259792327881
Loss: 2.2756965160369873
Loss: 2.308051824569702
Loss: 2.3039817810058594
Loss: 2.421257972717285
Loss: 2.2856342792510986
Loss: 3.4086174964904785
Loss: 2.4367198944091797
Loss: 2.1951544284820557
Loss: 1.9225777387619019
Loss: 2.4871468544006348
Loss: 2.61933970451355
Loss: 2.4737229347229004
Loss: 2.552665948867798
Loss: 1.8225373029708862
Loss: 2.1317667961120605
Loss: 2.268854856491089
Loss: 2.1620798110961914
Loss: 1.5395387411117554
Loss: 2.501314163208008
Loss: 2.9878249168395996
Loss: 2.0358030796051025
Loss: 3.1923089027404785
Loss: 2.796398878097534
Loss: 2.7137038707733154
Loss: 2.2460989952087402
Loss: 1.7645790576934814
Loss: 1.742877721786499
Loss: 1.6985281705856323
Loss: 2.1137185096740723
Loss: 2.2236578464508057
Loss: 2.4825549125671387
Loss: 2.459181308746338
Loss: 2.0904595851898193
Loss: 3.0297350883483887
Loss: 2.6945743560791016
Loss: 2.497025966644287
Loss: 2.017533540725708
Loss: 2.3852384090423584
Loss: 2.853053569793701
Loss: 3.0614981651306152
Loss: 2.078014850616455
Loss: 2.076425313949585
Loss: 2.036240816116333
Loss: 2.698383092880249
Loss: 2.4197463989257812
Loss: 2.021698236465454
Loss: 2.0481390953063965
Loss: 1.9312101602554321
Loss: 2.2738068103790283
Loss: 2.914588212966919
Loss: 2.0166923999786377
Loss: 1.777195692062378
Loss: 1.9287835359573364
Loss: 2.426891326904297
Loss: 1.738156795501709
Loss: 2.6879818439483643
Loss: 2.1309239864349365
Loss: 1.9704012870788574
Loss: 2.4768874645233154
Loss: 2.7059268951416016
Loss: 2.3374826908111572
Loss: 2.840529680252075
Loss: 2.3009302616119385
Loss: 2.3810791969299316
Loss: 1.8425804376602173
Loss: 2.3082706928253174
Loss: 4.113051414489746
Loss: 2.059361219406128
Loss: 2.494497299194336
[Train] Epoch 34, accuracy 0.9927951388888889
[Eval] Epoch 34, accuracy 0.9892361111111111
Loss: 2.2596919536590576
Loss: 2.506209373474121
Loss: 2.4071216583251953
Loss: 2.8900930881500244
Loss: 2.1818623542785645
Loss: 2.8586361408233643
Loss: 2.3334906101226807
Loss: 1.6389274597167969
Loss: 2.748353958129883
Loss: 2.261972427368164
Loss: 1.983312964439392
Loss: 1.7328709363937378
Loss: 2.2733724117279053
Loss: 2.729952335357666
Loss: 1.723573088645935
Loss: 2.48417329788208
Loss: 2.6217381954193115
Loss: 2.155751943588257
Loss: 1.9659231901168823
Loss: 2.4379451274871826
Loss: 2.0545012950897217
Loss: 2.9568634033203125
Loss: 2.3429248332977295
Loss: 1.8943922519683838
Loss: 2.45416522026062
Loss: 2.4165468215942383
Loss: 1.2947380542755127
Loss: 2.227499485015869
Loss: 1.7066408395767212
Loss: 1.6111210584640503
Loss: 1.9389420747756958
Loss: 2.3247547149658203
Loss: 2.1219024658203125
Loss: 1.7294678688049316
Loss: 2.832160234451294
Loss: 2.772658109664917
Loss: 1.8532670736312866
Loss: 2.0238664150238037
Loss: 2.1947526931762695
Loss: 2.174541711807251
Loss: 2.281311511993408
Loss: 2.2350051403045654
Loss: 2.1878671646118164
Loss: 2.0467581748962402
Loss: 3.034740686416626
Loss: 2.4459354877471924
Loss: 1.9624041318893433
Loss: 2.5756349563598633
Loss: 1.654893159866333
Loss: 2.482869863510132
Loss: 2.839723587036133
Loss: 2.012387990951538
Loss: 2.1492438316345215
Loss: 4.291745185852051
Loss: 1.820773720741272
Loss: 2.0983433723449707
Loss: 2.0905206203460693
Loss: 2.12426495552063
Loss: 2.0871217250823975
Loss: 1.9270563125610352
Loss: 2.2801408767700195
Loss: 2.5109100341796875
Loss: 2.3309285640716553
Loss: 2.285966157913208
Loss: 2.2429301738739014
Loss: 2.090394973754883
Loss: 2.061049461364746
Loss: 2.076099157333374
Loss: 1.7519676685333252
Loss: 2.043164014816284
Loss: 2.3029472827911377
Loss: 2.8277158737182617
Loss: 2.030754327774048
Loss: 2.028550863265991
Loss: 2.677966594696045
Loss: 2.713529109954834
Loss: 3.1030640602111816
Loss: 2.1130290031433105
Loss: 1.7296972274780273
Loss: 1.8899927139282227
Loss: 2.0784225463867188
Loss: 2.4065115451812744
Loss: 2.0181944370269775
Loss: 1.9767248630523682
Loss: 1.3628261089324951
Loss: 2.422780752182007
Loss: 2.6012730598449707
Loss: 2.3643321990966797
Loss: 2.7652413845062256
Loss: 2.8063764572143555
[Train] Epoch 35, accuracy 0.9918402777777777
[Eval] Epoch 35, accuracy 0.9916666666666667
Loss: 2.860736131668091
Loss: 2.1486117839813232
Loss: 1.8065577745437622
Loss: 1.7005598545074463
Loss: 2.547443389892578
Loss: 1.6402747631072998
Loss: 2.065436840057373
Loss: 1.851833701133728
Loss: 2.1819963455200195
Loss: 2.0208535194396973
Loss: 2.5911054611206055
Loss: 2.7787647247314453
Loss: 2.5669991970062256
Loss: 2.1302542686462402
Loss: 2.700906753540039
Loss: 1.7006760835647583
Loss: 2.829177141189575
Loss: 1.8376381397247314
Loss: 1.7404018640518188
Loss: 1.959214210510254
Loss: 1.9089462757110596
Loss: 2.8122775554656982
Loss: 2.955742359161377
Loss: 1.814558744430542
Loss: 3.078026056289673
Loss: 2.4221954345703125
Loss: 2.6014294624328613
Loss: 1.7285785675048828
Loss: 2.4391088485717773
Loss: 1.8898428678512573
Loss: 1.652816653251648
Loss: 1.842200517654419
Loss: 2.4838411808013916
Loss: 2.2783637046813965
Loss: 1.431147813796997
Loss: 1.9367051124572754
Loss: 1.8136767148971558
Loss: 2.5966837406158447
Loss: 1.6486179828643799
Loss: 1.9763160943984985
Loss: 1.8625372648239136
Loss: 3.002183198928833
Loss: 1.9127904176712036
Loss: 1.8081973791122437
Loss: 2.058030843734741
Loss: 2.253519296646118
Loss: 2.629768133163452
Loss: 1.6175695657730103
Loss: 2.3174941539764404
Loss: 2.037778615951538
Loss: 1.9318301677703857
Loss: 1.8903812170028687
Loss: 1.917441725730896
Loss: 2.4220643043518066
Loss: 2.3730990886688232
Loss: 2.5912604331970215
Loss: 2.594205379486084
Loss: 2.005098819732666
Loss: 2.200917959213257
Loss: 1.8052430152893066
Loss: 1.8258625268936157
Loss: 1.9381910562515259
Loss: 2.658858299255371
Loss: 2.1095805168151855
Loss: 1.9071124792099
Loss: 1.9694761037826538
Loss: 1.7226531505584717
Loss: 2.065650701522827
Loss: 2.128443479537964
Loss: 1.7387229204177856
Loss: 2.246565580368042
Loss: 2.093369722366333
Loss: 2.504988193511963
Loss: 1.5968823432922363
Loss: 2.4299700260162354
Loss: 1.707806944847107
Loss: 2.222719430923462
Loss: 1.2298718690872192
Loss: 2.2648837566375732
Loss: 1.685203194618225
Loss: 2.2254602909088135
Loss: 1.8952579498291016
Loss: 2.251750946044922
Loss: 2.02872896194458
Loss: 1.9904946088790894
Loss: 2.540877103805542
Loss: 1.7877110242843628
Loss: 2.0851974487304688
Loss: 2.503653049468994
Loss: 1.9500104188919067
[Train] Epoch 36, accuracy 0.9922743055555555
[Eval] Epoch 36, accuracy 0.9927083333333333
Loss: 1.699252963066101
Loss: 1.7473639249801636
Loss: 1.4130351543426514
Loss: 1.6679762601852417
Loss: 1.8585362434387207
Loss: 1.6394550800323486
Loss: 1.4654655456542969
Loss: 1.9423571825027466
Loss: 1.4850883483886719
Loss: 1.4227477312088013
Loss: 1.8230730295181274
Loss: 2.2968645095825195
Loss: 1.479853630065918
Loss: 2.2170510292053223
Loss: 1.818780541419983
Loss: 1.9516667127609253
Loss: 2.424097776412964
Loss: 2.0982506275177
Loss: 2.3866348266601562
Loss: 1.7371286153793335
Loss: 1.532632827758789
Loss: 2.535951852798462
Loss: 1.4623950719833374
Loss: 2.3678507804870605
Loss: 1.78788161277771
Loss: 2.206202983856201
Loss: 2.361704111099243
Loss: 1.7457737922668457
Loss: 2.263622283935547
Loss: 2.081793785095215
Loss: 1.5939844846725464
Loss: 2.334718942642212
Loss: 2.014610528945923
Loss: 1.5391011238098145
Loss: 2.2623884677886963
Loss: 1.9697147607803345
Loss: 3.0196022987365723
Loss: 2.5309810638427734
Loss: 1.6871426105499268
Loss: 2.304720401763916
Loss: 2.4357004165649414
Loss: 2.0065839290618896
Loss: 1.9595704078674316
Loss: 1.9833475351333618
Loss: 1.7129862308502197
Loss: 2.2904467582702637
Loss: 1.9906708002090454
Loss: 1.8459354639053345
Loss: 1.3648135662078857
Loss: 1.9664183855056763
Loss: 1.8070824146270752
Loss: 1.7083849906921387
Loss: 1.6186742782592773
Loss: 1.4752471446990967
Loss: 3.2826645374298096
Loss: 2.003779649734497
Loss: 1.8716281652450562
Loss: 2.4451193809509277
Loss: 1.9064751863479614
Loss: 2.2040300369262695
Loss: 2.1887850761413574
Loss: 1.7467938661575317
Loss: 1.8712784051895142
Loss: 1.6304340362548828
Loss: 1.7594512701034546
Loss: 2.395505428314209
Loss: 2.8010456562042236
Loss: 1.9050592184066772
Loss: 2.5074968338012695
Loss: 1.741303563117981
Loss: 2.7799630165100098
Loss: 1.8403089046478271
Loss: 1.2571214437484741
Loss: 4.332016944885254
Loss: 2.095803737640381
Loss: 1.694579839706421
Loss: 1.5462743043899536
Loss: 1.4513839483261108
Loss: 2.195878505706787
Loss: 3.3664443492889404
Loss: 2.2730493545532227
Loss: 1.6359347105026245
Loss: 1.8854889869689941
Loss: 1.706594705581665
Loss: 1.7139370441436768
Loss: 2.2601876258850098
Loss: 2.381714105606079
Loss: 1.8245539665222168
Loss: 1.2001615762710571
Loss: 2.263854503631592
[Train] Epoch 37, accuracy 0.9924479166666667
[Eval] Epoch 37, accuracy 0.9920138888888889
Loss: 1.8990085124969482
Loss: 1.6365152597427368
Loss: 1.9115418195724487
Loss: 1.5393006801605225
Loss: 1.9355385303497314
Loss: 2.752906322479248
Loss: 1.4583271741867065
Loss: 2.6660144329071045
Loss: 1.9150538444519043
Loss: 2.1897664070129395
Loss: 2.599696397781372
Loss: 1.8264636993408203
Loss: 2.0782113075256348
Loss: 2.0354862213134766
Loss: 1.7785555124282837
Loss: 2.371515989303589
Loss: 2.4160170555114746
Loss: 2.0851001739501953
Loss: 2.089543581008911
Loss: 1.6216468811035156
Loss: 1.9425286054611206
Loss: 1.5304116010665894
Loss: 2.115572929382324
Loss: 2.534031391143799
Loss: 1.911001443862915
Loss: 1.6789630651474
Loss: 1.518149495124817
Loss: 2.074831962585449
Loss: 1.4529558420181274
Loss: 2.061657190322876
Loss: 2.0624372959136963
Loss: 2.133394241333008
Loss: 2.19636607170105
Loss: 1.7609343528747559
Loss: 1.4749611616134644
Loss: 1.843315601348877
Loss: 2.4087555408477783
Loss: 1.723293662071228
Loss: 1.9752776622772217
Loss: 1.5649912357330322
Loss: 1.502049207687378
Loss: 2.089141845703125
Loss: 1.6480852365493774
Loss: 1.6412919759750366
Loss: 1.6866590976715088
Loss: 1.7544207572937012
Loss: 1.2499638795852661
Loss: 1.9336644411087036
Loss: 1.7676143646240234
Loss: 1.903944730758667
Loss: 1.6749558448791504
Loss: 1.6175094842910767
Loss: 2.535317897796631
Loss: 1.809262752532959
Loss: 2.0009589195251465
Loss: 2.6561427116394043
Loss: 1.582763910293579
Loss: 2.322218656539917
Loss: 1.6770012378692627
Loss: 1.6780283451080322
Loss: 1.8879541158676147
Loss: 2.351100206375122
Loss: 1.686591625213623
Loss: 1.6101837158203125
Loss: 2.243839740753174
Loss: 2.2293262481689453
Loss: 2.066467761993408
Loss: 1.696922779083252
Loss: 1.5356191396713257
Loss: 1.8805075883865356
Loss: 1.6595542430877686
Loss: 2.2200424671173096
Loss: 1.558498740196228
Loss: 2.2908318042755127
Loss: 1.7301000356674194
Loss: 2.729990243911743
Loss: 1.4968279600143433
Loss: 2.595956563949585
Loss: 2.0842299461364746
Loss: 2.66294264793396
Loss: 2.883700370788574
Loss: 1.5635309219360352
Loss: 2.0424587726593018
Loss: 2.015726089477539
Loss: 1.4494202136993408
Loss: 1.647221326828003
Loss: 1.7923895120620728
Loss: 1.7045656442642212
Loss: 2.2448627948760986
Loss: 2.2160089015960693
[Train] Epoch 38, accuracy 0.9927083333333333
[Eval] Epoch 38, accuracy 0.9927083333333333
Loss: 1.9918742179870605
Loss: 1.4910837411880493
Loss: 2.8407657146453857
Loss: 1.8824172019958496
Loss: 1.6833299398422241
Loss: 1.9537111520767212
Loss: 1.636744499206543
Loss: 3.0848913192749023
Loss: 2.3120718002319336
Loss: 2.120605707168579
Loss: 2.1992502212524414
Loss: 1.5336980819702148
Loss: 2.4216511249542236
Loss: 2.0398635864257812
Loss: 2.272172212600708
Loss: 2.5352120399475098
Loss: 2.2574543952941895
Loss: 1.893911600112915
Loss: 2.0336995124816895
Loss: 0.8734027743339539
Loss: 1.7935696840286255
Loss: 1.9031907320022583
Loss: 2.5556955337524414
Loss: 1.4083255529403687
Loss: 2.6308445930480957
Loss: 2.03340482711792
Loss: 2.719938278198242
Loss: 1.8332693576812744
Loss: 2.3307294845581055
Loss: 2.08896803855896
Loss: 2.077148199081421
Loss: 1.7171579599380493
Loss: 1.8465867042541504
Loss: 1.6222740411758423
Loss: 1.7988449335098267
Loss: 2.027923822402954
Loss: 1.3953717947006226
Loss: 1.7107877731323242
Loss: 1.6318516731262207
Loss: 1.5437192916870117
Loss: 1.856236219406128
Loss: 1.4413001537322998
Loss: 1.7449381351470947
Loss: 1.6242167949676514
Loss: 1.5386911630630493
Loss: 2.2038991451263428
Loss: 2.0615925788879395
Loss: 2.0003156661987305
Loss: 1.4103288650512695
Loss: 1.806199312210083
Loss: 1.5397621393203735
Loss: 1.931478500366211
Loss: 1.6956799030303955
Loss: 1.958522081375122
Loss: 1.7374024391174316
Loss: 2.097700834274292
Loss: 2.1585912704467773
Loss: 1.9520044326782227
Loss: 1.482812762260437
Loss: 1.6414823532104492
Loss: 1.6738169193267822
Loss: 1.7449324131011963
Loss: 1.6410874128341675
Loss: 1.7274221181869507
Loss: 1.9379347562789917
Loss: 2.135373592376709
Loss: 1.4718375205993652
Loss: 1.5010634660720825
Loss: 2.2075469493865967
Loss: 1.244490146636963
Loss: 2.571502685546875
Loss: 1.9930793046951294
Loss: 1.1568942070007324
Loss: 2.0936062335968018
Loss: 1.713820219039917
Loss: 2.063678026199341
Loss: 1.6066358089447021
Loss: 1.3682438135147095
Loss: 1.8781569004058838
Loss: 1.5755279064178467
Loss: 1.419602394104004
Loss: 2.3053901195526123
Loss: 1.460109829902649
Loss: 2.0046160221099854
Loss: 2.0360891819000244
Loss: 1.4327987432479858
Loss: 1.6772738695144653
Loss: 2.050964593887329
Loss: 1.1634416580200195
Loss: 2.5027341842651367
[Train] Epoch 39, accuracy 0.9933159722222222
[Eval] Epoch 39, accuracy 0.9947916666666666
Model saved as x_small_model_weights_best.pth
Loss: 2.198150634765625
Loss: 2.006551742553711
Loss: 1.74603271484375
Loss: 2.236704111099243
Loss: 1.5560132265090942
Loss: 2.086486339569092
Loss: 2.1010794639587402
Loss: 1.915412187576294
Loss: 1.3865430355072021
Loss: 2.418287515640259
Loss: 2.004364252090454
Loss: 1.9399758577346802
Loss: 2.6104915142059326
Loss: 1.9111638069152832
Loss: 2.0533673763275146
Loss: 2.298734664916992
Loss: 2.0166945457458496
Loss: 2.240539073944092
Loss: 1.7764053344726562
Loss: 1.8081209659576416
Loss: 2.017124652862549
Loss: 1.5980263948440552
Loss: 1.5584546327590942
Loss: 2.2999305725097656
Loss: 2.1341564655303955
Loss: 1.4753429889678955
Loss: 1.5447479486465454
Loss: 1.6559734344482422
Loss: 1.727845549583435
Loss: 2.4005675315856934
Loss: 1.401864767074585
Loss: 1.553127646446228
Loss: 2.1041643619537354
Loss: 3.207052707672119
Loss: 2.093257188796997
Loss: 2.3675460815429688
Loss: 1.4649291038513184
Loss: 1.8749501705169678
Loss: 1.6948153972625732
Loss: 2.422008514404297
Loss: 1.452146053314209
Loss: 1.5240848064422607
Loss: 2.1617588996887207
Loss: 1.633554458618164
Loss: 1.9362612962722778
Loss: 1.9990148544311523
Loss: 1.3915469646453857
Loss: 2.0825860500335693
Loss: 1.6342931985855103
Loss: 1.3774465322494507
Loss: 1.1958483457565308
Loss: 1.8473056554794312
Loss: 2.1028048992156982
Loss: 1.1255472898483276
Loss: 1.7050803899765015
Loss: 2.2879347801208496
Loss: 2.055867910385132
Loss: 1.3197346925735474
Loss: 1.7398138046264648
Loss: 3.771543502807617
Loss: 2.0405914783477783
Loss: 1.5027859210968018
Loss: 1.4196085929870605
Loss: 1.5881385803222656
Loss: 1.8856425285339355
Loss: 1.590590238571167
Loss: 2.1146812438964844
Loss: 1.7547203302383423
Loss: 1.815303921699524
Loss: 1.886494755744934
Loss: 2.0885987281799316
Loss: 2.022223711013794
Loss: 1.5274977684020996
Loss: 1.619776964187622
Loss: 1.5299221277236938
Loss: 2.0362868309020996
Loss: 1.3389736413955688
Loss: 1.7489731311798096
Loss: 1.808555245399475
Loss: 2.7476806640625
Loss: 1.4069442749023438
Loss: 2.312333822250366
Loss: 1.98243248462677
Loss: 1.790886640548706
Loss: 2.0953822135925293
Loss: 1.8333405256271362
Loss: 1.7025189399719238
Loss: 2.5189409255981445
Loss: 1.588572382926941
Loss: 1.4542341232299805
[Train] Epoch 40, accuracy 0.9916666666666667
[Eval] Epoch 40, accuracy 0.9913194444444444
Best accuracy: 0.995
